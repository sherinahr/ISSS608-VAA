[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello there, my name is Sherinah!\nI am currently taking the Visual Analytics and Applications course in SMU.\nIn this website, you will find my attempts at completing the coursework, and I hope you will find it informative and aesthetically pleasing!"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launch-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launch-r-packages",
    "title": "Hands-on Exercise 1",
    "section": "1.2.1 Install and launch R packages",
    "text": "1.2.1 Install and launch R packages\nBefore we get started, it is important for us to ensure that the required R packages have been installed. If yes, we will load the R packages. If they have yet to be installed, we will install the R packages and load them onto R environment.\nThe code chunk below uses p_load() of pacman package to check if tidyverse package is installed.\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "title": "Hands-on Exercise 1",
    "section": "1.2.2 Importing the Data",
    "text": "1.2.2 Importing the Data\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, part of the tidyverse package.\n\n\nexam_data <-read_csv(\"data/Exam_data.csv\")\n\n\nYear end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#r-graphics-vs-ggplot",
    "title": "Hands-on Exercise 1",
    "section": "1.3.1 R Graphics vs ggplot",
    "text": "1.3.1 R Graphics vs ggplot\nLet’s compare how R Graphics and ggplot a simple histogram.\n\nR Graphicsggplot\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"white\", \n                 fill=\"lightpink2\") +\n  ggtitle(\"Distribution of Maths Scores\") +theme_classic() \n\n\n\n\n\n\n\nAs you can see that the code chunk is relatively simple if R Graphics is used. Then, the question is why ggplot2 is recommended?\n\n\n\n\n\n\nImportant\n\n\n\nThe transferable skills from ggplot2 are not the idiosyncrasies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "title": "Hands-on Exercise 1",
    "section": "1.4.1 A Layered Grammar of Graphics",
    "text": "1.4.1 A Layered Grammar of Graphics\nggplot2 is an implementation of Leland Wilkinson’s Grammar of Graphics. There are seven grammars of ggplot2:\n\nData: The dataset being plotted\nAesthetics take attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: The visual elements used for our data, such as point, bar or line\nFacets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots)\nStatistics, statistical transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic\nThemes modify all non-data components of a plot, such as main title, sub-title, y-axis title, or legend background"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_bar",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_bar",
    "title": "Hands-on Exercise 1",
    "section": "1.7.1 Geometric Objects: geom_bar",
    "text": "1.7.1 Geometric Objects: geom_bar\nThe code chunk below plots a bar chart by using geom_bar().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_dotplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_dotplot",
    "title": "Hands-on Exercise 1",
    "section": "1.7.2 Geometric Objects: geom_dotplot",
    "text": "1.7.2 Geometric Objects: geom_dotplot\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked, with each dot representing one observation.\nIn the code chunk below, geom_dotplot() of ggplot2 is used to plot a dot plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe y-axis scale is not very useful and in fact, is very misleading.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk below performs the following two steps:\n\nscale_y_continuous() is used to turn off the y-axis, and\nbinwidth argument is used to change the binwidth to 2.5\n\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_histogram",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_histogram",
    "title": "Hands-on Exercise 1",
    "section": "1.7.3 Geometric Objects: geom_histogram",
    "text": "1.7.3 Geometric Objects: geom_histogram\nIn the code chunk below, geom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()       \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the default bin is 30."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-geom",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-geom",
    "title": "Hands-on Exercise 1",
    "section": "1.7.4 Modifying a geometric object by changing geom",
    "text": "1.7.4 Modifying a geometric object by changing geom\nIn the code chunk below:\n\nbins argument is used to change the number of bins to 20,\nfill argument is used to shade the histogram with pastel blue color,\ncolor argument is used to change the outline colour of the bars in black, and\ntheme_classic() is used to make the background minimalist.\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"white\",      \n                 fill=\"slategray1\") + theme_classic()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-aes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#modifying-a-geometric-object-by-changing-aes",
    "title": "Hands-on Exercise 1",
    "section": "1.7.5 Modifying a geometric object by changing aes",
    "text": "1.7.5 Modifying a geometric object by changing aes\nThe code chunk below changes the interior colour of the histogram (i.e. fill) by using sub-group of aesthetic().\n\nlibrary(viridis)  \n\nLoading required package: viridisLite\n\nggplot(data=exam_data, \n       aes(x= MATHS, fill= GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\") +   theme_classic() +\n  scale_fill_hue(l=80, c=80)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach can be used to colour and fill of the geometric object."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom-density",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom-density",
    "title": "Hands-on Exercise 1",
    "section": "1.7.6 Geometric Objects: geom-density",
    "text": "1.7.6 Geometric Objects: geom-density\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density estimate plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()           \n\n\n\n\nThe code chunk below plots two kernel density lines by using colour or fill arguments of aes().\n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density() +\n  theme_classic()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_boxplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_boxplot",
    "title": "Hands-on Exercise 1",
    "section": "1.7.7 Geometric Objects: geom_boxplot",
    "text": "1.7.7 Geometric Objects: geom_boxplot\ngeom_boxplot() displays the list of continuous values. It visualises five summary statistics (the median, two hinges and two whiskers), and all outlier points individually.\nThe code chunk below plots boxplots by using geom_boxplot().\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER, fill = GENDER)) +    \n  geom_boxplot() + theme_classic() +\n  theme(legend.position=\"none\") +\n  scale_fill_hue(l=80, c=80)\n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\nThe code chunk below plots the distribution of Maths scores by gender in notched plot instead of boxplot.\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER, fill = GENDER)) +    \n  geom_boxplot(notch=TRUE) + theme_classic() +\n  theme(legend.position=\"none\") +\n  scale_fill_hue(l=80, c=80)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_violin",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_violin",
    "title": "Hands-on Exercise 1",
    "section": "1.7.8 Geometric Objects: geom_violin",
    "text": "1.7.8 Geometric Objects: geom_violin\ngeom_violin is designed for creating violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plot the distribution of Maths score by gender in a violin plot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER, fill = GENDER)) +\n  geom_violin() + theme_classic() +\n  theme(legend.position=\"none\") +\n  scale_fill_hue(l=80, c=80)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_point",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometric-objects-geom_point",
    "title": "Hands-on Exercise 1",
    "section": "1.7.9 Geometric Objects: geom_point",
    "text": "1.7.9 Geometric Objects: geom_point\ngeom_point() is especially useful for creating scatterplot.\nThe code chunk below plots a scatterplot showing the Maths and English grades of pupils by using geom_point().\n\nggplot(data=exam_data, \n       aes(x= MATHS, y = ENGLISH, color=GENDER)) +\n  geom_point() + theme_classic()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geom-objects-can-be-combined",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geom-objects-can-be-combined",
    "title": "Hands-on Exercise 1",
    "section": "1.7.10 geom objects can be combined",
    "text": "1.7.10 geom objects can be combined\nThe code chunk below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER, fill=GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5) + theme_classic() +\n  scale_fill_hue(l=80, c=80)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---stat_summary-method",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---stat_summary-method",
    "title": "Hands-on Exercise 1",
    "section": "1.8.1 Working with stat() - stat_summary method",
    "text": "1.8.1 Working with stat() - stat_summary method\nThe boxplots above in Section 1.7.7 were incomplete because the positions of the means were not shown. The code chunk below adds mean values by using stat_summary() function and overriding the default geom.\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER, fill = GENDER)) +    \n  geom_boxplot() + theme_classic() +\n  stat_summary(geom = \"point\",       \n               fun =\"mean\",         \n               colour =\"black\",        \n               size=2)+\n  theme(legend.position=\"none\") +\n  scale_fill_hue(l=80, c=80)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---geom-method",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-stat---geom-method",
    "title": "Hands-on Exercise 1",
    "section": "1.8.2 Working with stat() - geom() method",
    "text": "1.8.2 Working with stat() - geom() method\nThe code chunk below adding mean values by using geom() function and overriding the default stat.\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER, fill = GENDER)) +    \n  geom_boxplot() + theme_classic() +\n  geom_point(stat=\"summary\",        \n             fun=\"mean\",           \n             colour =\"black\",          \n             size=2) +\n  theme(legend.position=\"none\") +\n  scale_fill_hue(l=80, c=80)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#adding-a-best-fit-curve-on-a-scatterplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#adding-a-best-fit-curve-on-a-scatterplot",
    "title": "Hands-on Exercise 1",
    "section": "1.8.3 Adding a best-fit curve on a scatterplot",
    "text": "1.8.3 Adding a best-fit curve on a scatterplot\nThe scatterplot above in Section 1.7.9 showed the relationship of Maths and English grades of pupils. The interpretability of the graph can be improved by adding a best fit curve. In the code chunk below, geom_smooth() is used to plot a best fit curve on the scatterplot.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y = ENGLISH, fill = GENDER)) +\n  geom_point() + theme_classic() +\n  geom_smooth(linewidth=0.5)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default method used is loess.\n\n\nThe default smoothing method can be overridden as shown below.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y = ENGLISH, fill = GENDER)) +\n  geom_point() + theme_classic() +\n  geom_smooth(method=lm, linewidth=0.5)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-facet_wrap",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-facet_wrap",
    "title": "Hands-on Exercise 1",
    "section": "1.9.1 Working with facet_wrap()",
    "text": "1.9.1 Working with facet_wrap()\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.The code chunk below plots a trellis plot using facet-wrap().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20, fill=\"salmon\") + theme_classic() +\n  facet_wrap(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-facet_grid",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-facet_grid",
    "title": "Hands-on Exercise 1",
    "section": "1.9.2 Working with facet_grid()",
    "text": "1.9.2 Working with facet_grid()\nfacet_grid() forms a matrix of panels defined by row and column variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.The code chunk below plots a trellis plot using facet_grid().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20, fill=\"salmon\") + theme_classic() +\n  facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-coordinate",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-coordinate",
    "title": "Hands-on Exercise 1",
    "section": "1.10.1 Working with Coordinate",
    "text": "1.10.1 Working with Coordinate\nBy default, the bar chart of ggplot2 is in vertical form. Adding coord_flip() flips the horizontal bar chart into a vertical bar chart.\n\nggplot(data = exam_data, \n       aes(x = RACE, fill= RACE)) + \n  geom_bar() +\n  xlab (\"Race\") +\n  ylab(\"Number of Students\") +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  theme_classic() + \n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#changing-the-y--and-x-axis-range",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#changing-the-y--and-x-axis-range",
    "title": "Hands-on Exercise 1",
    "section": "1.10.2 Changing the y- and x-axis range",
    "text": "1.10.2 Changing the y- and x-axis range\nThe scatterplot in Section 1.7.9 is slightly misleading because the y-axis and x-axis ranges are not equal. See below for the amended code block.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y = ENGLISH, fill = GENDER)) +\n  geom_point() + theme_classic() +\n  geom_smooth(method=lm, linewidth=0.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on_Ex02",
    "section": "2.2.1 Installing and loading the required libraries",
    "text": "2.2.1 Installing and loading the required libraries\nIn this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\nCode chunk below will be used to check if these packages have been installed and also will load them onto your working R environment.\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data",
    "title": "Hands-on_Ex02",
    "section": "2.2.2 Importing data",
    "text": "2.2.2 Importing data\nFor the purpose of this exercise, as in Hands-on Exercise 1, a data file called Exam_data will be used.\n\nexam_data <-read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#working-with-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#working-with-ggrepel",
    "title": "Hands-on_Ex02",
    "section": "2.3.1 Working with ggrepel",
    "text": "2.3.1 Working with ggrepel\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#working-with-ggtheme-package",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#working-with-ggtheme-package",
    "title": "Hands-on_Ex02",
    "section": "2.4.1 Working with ggtheme package",
    "text": "2.4.1 Working with ggtheme package\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others. In the example below, The Economist theme is used.It also provides some extra geoms and scales for ‘ggplot2’. Consult this vignette to learn more.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_economist() +\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#working-with-hrbthemes-package",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#working-with-hrbthemes-package",
    "title": "Hands-on_Ex02",
    "section": "2.4.2 Working with hrbthemes package",
    "text": "2.4.2 Working with hrbthemes package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used. Consult this vignette to learn more.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\n\n\n\nThe axis_title_size argument is used to increase the font size of the axis title to 18\nThe base_size argument is used to increase the default axis label to 15\nThe grid argument is used to remove the x-axis grid lines"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-composite-graphics-patchwork-methods",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-composite-graphics-patchwork-methods",
    "title": "Hands-on_Ex02",
    "section": "2.5.1 Creating Composite Graphics: Patchwork methods",
    "text": "2.5.1 Creating Composite Graphics: Patchwork methods\nThere are several ggplot2 extension’s functions to support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package. In this section, an ggplot2 extension called patchwork will be used to combine separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-two-ggplot2-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-two-ggplot2-graphs",
    "title": "Hands-on_Ex02",
    "section": "2.5.2 Combining two ggplot2 graphs",
    "text": "2.5.2 Combining two ggplot2 graphs\nFigure in the tabset below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot is!\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np1 + p2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-three-ggplot2-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-three-ggplot2-graphs",
    "title": "Hands-on_Ex02",
    "section": "2.5.3 Combining three ggplot2 graphs",
    "text": "2.5.3 Combining three ggplot2 graphs\nWe can plot more complex composites by using appropriate operators. For example, the composite figure below is plotted by using:\n\n“|” operator to stack two ggplot2 graphs\n“/” operator to place the plots beside each other\n“()” operator the define the sequence of the plotting\n\nTo learn more about, refer to Plot Assembly.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) &\n  theme(title=element_text(size=7, face ='bold'))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-with-tag",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-with-tag",
    "title": "Hands-on_Ex02",
    "section": "2.5.4 Creating a composite figure with tag",
    "text": "2.5.4 Creating a composite figure with tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\ngrid <- ((p1 / p2) | p3) + plot_annotation(tag_levels = 'I') \ngrid & theme(title=element_text(size=7, face ='bold'))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-figure-with-insert",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-figure-with-insert",
    "title": "Hands-on_Ex02",
    "section": "2.5.5 Creating figure with insert",
    "text": "2.5.5 Creating figure with insert\nBeside providing functions to place plots next to each other based on the provided layout, with inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np12 <- p1|p2\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-by-using-patchwork-and-ggtheme",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-by-using-patchwork-and-ggtheme",
    "title": "Hands-on_Ex02",
    "section": "2.5.6 Creating a composite figure by using patchwork and ggtheme",
    "text": "2.5.6 Creating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\npatchwork <- (p1 / p2) | p3\npatchwork & theme_economist() + theme(title=element_text(size = 6, face ='bold'), \n                                      axis.title.y=element_text(size = 9), axis.title.x=element_text(size = 9))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "Using p_load() of pacman package to load tidyverse.\n\npacman::p_load(tidyverse)\n\nImporting the data.\n\nexam_data <-read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-theme",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-theme",
    "title": "In-class Exercise 1",
    "section": "1.2 Working with theme",
    "text": "1.2 Working with theme\n\nChanging the colors of plot panel background of theme_minimal() to light blue and the color of grid lines to white.\n\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() + \n  theme(panel.background = element_rect(fill = \"#ADD8E6\",\n                                linewidth = 2, linetype = \"solid\"),\n  panel.grid.major = element_line(linewidth = 0.5, \n                                  linetype = 'solid',\n                                colour = \"white\"), \n  panel.grid.minor = element_line(linewidth = 0.25, \n                                  linetype = 'solid',\n                                colour = \"white\"))+\n  coord_flip()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-data-drive-graphics-for-analysis-i",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-data-drive-graphics-for-analysis-i",
    "title": "In-class Exercise 1",
    "section": "1.3 Designing Data-drive Graphics for Analysis I",
    "text": "1.3 Designing Data-drive Graphics for Analysis I\n\n1.3.1 The original design\nA simple vertical bar chart for frequency analysis. Critical analysis:\n\ny-axis label is not clear (i.e., count)\nTo support effective comparison, the bars should be sorted by their respective frequencies.\nFor static graph, frequency values should be added to provide additional information.\n\n\n\n\n\n\n\n\n1.3.2 Makeover!\n\nThe makeover designThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x= reorder (RACE,RACE,\n                           function(x)-length(x)))) +\n  geom_bar() + \n  ylim(0, 220) +\n  xlab (\"Race\") +\n  ylab(\"Number of Students\") +\n  geom_text(stat=\"count\", \n      aes(label=paste0(..count.., \", \", \n      round(..count../sum(..count..)*100, 1), \"%\")),\n      vjust=-1)\n\n\n\n\nAlternatively, this code chunk uses fct_infreq() of forcats package.\n\nexam_data %>%\n  mutate(RACE = fct_infreq(RACE)) %>%\n  ggplot(aes(x = RACE)) + \n  geom_bar()+\n  ylim(0,220) +\n  geom_text(stat=\"count\", \n      aes(label=paste0(..count.., \", \", \n      round(..count../sum(..count..)*100,\n            1), \"%\")),\n      vjust=-1) +\n  xlab(\"Race\") +\n  ylab(\"No. of\\nPupils\") +\n  theme(axis.title.y=element_text(angle = 0))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-data-drive-graphics-for-analysis-ii",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-data-drive-graphics-for-analysis-ii",
    "title": "In-class Exercise 1",
    "section": "1.4 Designing Data-drive Graphics for Analysis II",
    "text": "1.4 Designing Data-drive Graphics for Analysis II\n\n1.4.1 The original design\nA histogram of the Math scores:\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20)    \n\n\n\n\n\n\n1.4.2 Makeover!\n\nThe makeover designThe code chunk\n\n\n\nAdding mean and median lines on the histogram plot.\nChange fill and line color\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20, fill=\"lightblue\", color =\"black\") +\n  geom_vline(aes(xintercept=mean(MATHS),\n            color=\"Mean\"), linetype=\"dashed\", size=1) + \n    geom_vline(aes(xintercept=median(MATHS),\n            color=\"Median\"), linetype=\"dashed\", size=1) +\n    scale_color_manual(name = \"Statistics\", \n                       values = c(Median = \"blue\", Mean = \"red\")) +\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-data-drive-graphics-for-analysis-iii",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-data-drive-graphics-for-analysis-iii",
    "title": "In-class Exercise 1",
    "section": "1.5 Designing Data-drive Graphics for Analysis III",
    "text": "1.5 Designing Data-drive Graphics for Analysis III\n\n1.5.1 The original design\nThe histograms below are elegantly designed but not informative. This is because they only reveal the distribution of English scores by gender but without context such as all pupils.\n\nggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_histogram() +\n  facet_wrap(~ GENDER) \n\n\n\n\n\n\n1.5.2 Makeover!\n\nThe makeover designThe code chunk\n\n\nThe background histograms show the distribution of English scores for all pupils.\n\n\n\n\n\n\n\n\nd_bg <- exam_data[, -3]  \n\nggplot(exam_data, aes(x = ENGLISH, fill = GENDER)) +\n  geom_histogram(data = d_bg, fill = \"grey\", alpha = .5) +\n  geom_histogram(colour = \"black\") +\n  facet_wrap(~ GENDER) +\n  guides(fill = FALSE) +  \n  theme_bw()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-data-drive-graphics-for-analysis-iv",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-data-drive-graphics-for-analysis-iv",
    "title": "In-class Exercise 1",
    "section": "1.6 Designing Data-drive Graphics for Analysis IV",
    "text": "1.6 Designing Data-drive Graphics for Analysis IV\n\n1.6.1 The original design\n\nggplot(data=exam_data, \n       aes(x= MATHS, y = ENGLISH)) +\n  geom_point()\n\n\n\n\n\n\n1.6.2 Makeover!\n\nThe makeover designThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, y = ENGLISH)) +\n  geom_point() +\n  xlim(0,100) + \n  ylim(0,100) +\n  geom_hline(yintercept = 50, linetype=\"dashed\", \n             color = \"gray\", size=1) +\n  geom_vline(xintercept = 50, linetype=\"dashed\", \n             color = \"gray\", size=1)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to Sherinah’s page for Visual Analytics and Applications!\nIn this website, you will find my attempts at completing the coursework, and I hope you will find it informative and aesthetically pleasing~"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html",
    "title": "Demographic & Financial Analysis of City of Engagement",
    "section": "",
    "text": "City of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The main agriculture of the region is fruit farms and vineyards. The local council of the city is in the process of preparing the Local Plan 2023. A sample survey of 1000 representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things. The city aims to use the data to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received.\nThis take-home exercise will explore the demographic and financial characteristics of the city of Engagement using static and interactive statistical graphics methods. The data will be processed using the tidyverse family of packages and the visualisations developed using ggplot2 and its extensions.\nSpecifically, this exercise seeks to answer the following using Exploratory & Confirmatory Data Analysis: What influences happiness?\n\n\n\nTwo data sets in csv format are provided for the purpose of this exercise - Participants & Financial Journal.\n\n\nThis dataset contains information about the residents of City of Engagement that have agreed to participate in this study and comprises the following fields:\n\nData Fields - Participants.csv\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nparticipantId (integer)\nUnique ID assigned to each participant\n\n\nhouseholdSize (integer)\nNumber of people in the household\n\n\nhaveKids (boolean)\nPresence of children in the household\n\n\nage (integer)\nParticipant’s age in years at the start of the study\n\n\neducationLevel\n(string factor)\nParticipant’s education level\n[Low, HighSchoolorCollege, Bachelors, Graduate]\n\n\ninterestGroup (char)\nCharacter representing the participant’s stated primary interest group [A,B,C,D,E,F,G,H,I,J]\n\n\njoviality (float)\nValue ranging from [0,1] indicating the participant’s overall happiness level at the start of the study\n\n\n\n\n\n\nThis dataset contains information about the residents of City of Engagement that have agreed to participate in this study and comprises the following fields:\n\nData Fields - FinancialJournal.csv\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nparticipantId (integer)\nUnique ID assigned to each participant\n\n\ntimestamp (datetime)\nTime when the check-in was logged\n\n\ncategory\n(string factor)\nString describing the expense category\n[Education, Food, Recreation, RentAdjustment, Shelter, Wage]\n\n\namount (double)\nAmount of the transaction\n\n\n\n\n\n\nThere are a few data quality issues with the datasets:\n\nThe timestamp information in the FinancialJournal.csv is too granular and should be changed to reflect the month and year instead.\nA quick analysis showed that there were duplicates in the data; these will be removed to ensure the data will not be skewed.\nThe quick analysis also highlighted that there were some participants who did not have entries for all 6 months; it is possible that they are no longer residents of the City. Since the aim of this exercise is to explore the financial trends of the residents, these participants will be removed.\nThe data is on an individual level and thus will be aggregated accordingly for the analyses.\nThe data types have to be checked as well.\nAfter addressing the above issues, the datasets should be combined to ensure a comprehensive and accurate analysis.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to ensure that the datasets have been comprehensively cleaned prior to conducting analyses, to ensure our visualisations are accurate and informative!"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-preparation",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-preparation",
    "title": "Demographic & Financial Analysis of City of Engagement",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n2.1 Install and Launch R packages\nIn this exercise, the following packages will be used: tidyverse, ggplot2, dplyr, patchwork, ggthemes, hrbrthemes, and ggrepel. The codechunk below will be used to check if these packages have been installed and also will load them onto the working R environment:\n\n\nShow code\npacman::p_load(patchwork, ggplot2, dplyr, rstatix, gt,\n               gganimate, gifski, png, zoo, DT, ggjoy, PMCMRplus,\n               tidyverse, ggstatsplot, plotly) \n\n\n\n\n2.2 Import Datasets\nThe code chunk below imports the datasets into R environment by using read_csv() function of readr package as part of tidyverse.\n\n\nShow code\nppt <-read_csv(\"data/Participants.csv\", show_col_types = FALSE)\nfj <- read_csv(\"data/FinancialJournal.csv\", show_col_types = FALSE)\n\n\nThe summary and spec() function is used to provide an overview of the central tendencies, and data types of the datasets.\n\n\nShow code\nsummary(ppt)\n\n\n participantId    householdSize    haveKids            age       \n Min.   :   0.0   Min.   :1.000   Mode :logical   Min.   :18.00  \n 1st Qu.: 252.5   1st Qu.:1.000   FALSE:710       1st Qu.:29.00  \n Median : 505.0   Median :2.000   TRUE :301       Median :39.00  \n Mean   : 505.0   Mean   :1.964                   Mean   :39.07  \n 3rd Qu.: 757.5   3rd Qu.:3.000                   3rd Qu.:50.00  \n Max.   :1010.0   Max.   :3.000                   Max.   :60.00  \n educationLevel     interestGroup        joviality       \n Length:1011        Length:1011        Min.   :0.000204  \n Class :character   Class :character   1st Qu.:0.240074  \n Mode  :character   Mode  :character   Median :0.477539  \n                                       Mean   :0.493794  \n                                       3rd Qu.:0.746819  \n                                       Max.   :0.999234  \n\n\nShow code\nspec(ppt)\n\n\ncols(\n  participantId = col_double(),\n  householdSize = col_double(),\n  haveKids = col_logical(),\n  age = col_double(),\n  educationLevel = col_character(),\n  interestGroup = col_character(),\n  joviality = col_double()\n)\n\n\n\n\nShow code\nsummary(fj)\n\n\n participantId      timestamp                        category        \n Min.   :   0.0   Min.   :2022-03-01 00:00:00.00   Length:1513636    \n 1st Qu.: 222.0   1st Qu.:2022-05-24 13:25:00.00   Class :character  \n Median : 464.0   Median :2022-08-25 15:00:00.00   Mode  :character  \n Mean   : 480.9   Mean   :2022-08-26 05:00:48.42                     \n 3rd Qu.: 726.0   3rd Qu.:2022-11-27 07:25:00.00                     \n Max.   :1010.0   Max.   :2023-02-28 23:55:00.00                     \n     amount         \n Min.   :-1562.726  \n 1st Qu.:   -5.594  \n Median :   -4.000  \n Mean   :   20.047  \n 3rd Qu.:   21.598  \n Max.   : 4096.526  \n\n\nShow code\nspec(fj)\n\n\ncols(\n  participantId = col_double(),\n  timestamp = col_datetime(format = \"\"),\n  category = col_character(),\n  amount = col_double()\n)\n\n\n\n\n2.3 Data Wrangling\n\n2.3.1 Participants Dataset\nThere are a few data quality issues:\nFirstly, the age variable is binned with the following code chunk. The mutate() function of dplyr is used to create a new variable called Age_Category while preserving the existing variable of age.\n\n\nShow code\nppt <- ppt %>%\n  mutate(Age_Category=cut(age, \n                          breaks=c(17, 20, 30, 40, 50, 60),\n                          labels=c('20 & Below', '21-30', '31-40', \n                                   '41-50', '51 and above')))\n\n# Check if the ages are reflected correctly\nhead(ppt$age)\nhead(ppt$Age_Category)\n\n\nNext, a few of the columns were not set to the appropriate data type. For example, participantID should be categorical but it shows as a continuous numeric variable. Further, we would need the education level to be read as an ordinal factor as there are levels for education. As above, the mutate() function will be used to update the data types.\n\n\nShow code\nppt <- ppt %>% mutate(participantId = as.factor(participantId))\nppt <- ppt %>% mutate(educationLevel = factor(educationLevel, \n                                              levels=c(\"Low\", \"HighSchoolOrCollege\", \"Bachelors\", \"Graduate\")))\n\n# Check data types\nstr(ppt)\n\n\nLastly, we will rename the other columns for nicer formatting and ease of reading with the below code chunk. As we are not making changes to the values, the column names will be amended in place.\n\n\nShow code\nppt <- ppt %>%\n  rename('ParticipantID' = 'participantId', \n         'HouseholdSize' = 'householdSize', \n         'EducationLevel' = 'educationLevel', \n         'InterestGroup' = 'interestGroup', \n         'Joviality' = 'joviality',\n         'HaveKids' = 'haveKids')\n\n# Check if the changes are reflected correctly\ncolnames(ppt)\n\n\n\n\n2.3.2 FinancialJournal Dataset\nAs stated in section 1.2.3 Challenges, there are a number of issues with the dataset.\nFirstly, the data type for participantId was changed as above.\n\n\nShow code\nfj <- fj %>% mutate(participantId = as.factor(participantId))\n\n# Check data types\nstr(fj)\n\n\nNext, there are some duplicate rows in the dataset which have to be removed using the distinct() function. As the code below shows, there were 1,113 duplicate rows in the FinancialJournal dataset.\n\n\nShow code\nfj_2 <- fj %>% distinct()\nnrow(fj) - nrow(fj_2)\n\n\n[1] 1113\n\n\nNext, there were some participants who did not have entries for all 6 months. To find the these participants, first the data was grouped by the participantId. Examination of the results show that there are 131 participants which have lower than 1,000 transactions.\n\n\nShow code\nfj_2 %>%\n  group_by(participantId) %>%\n  summarize(transaction_count = n()) %>%\n  arrange(transaction_count)\n\n\nThe code chunk below will filter out these 131 participants. The data frame fj_final now has no duplicates, and only residents who still live in City of Engagement.\n\n\nShow code\nfj_final <- subset(fj_2, with(fj_2, participantId %in% \n                            names(which(table(participantId)>=1000))))\nhead(fj_final)\n\n\nNext, we would want to convert all of the timestamp data into Month-Year format for ease of comparison using the first line of code. However, the result would be in character format which is not advisable for analysis, and so we will run the subsequent line of code to change it to Date format.\n\n\nShow code\nfj_final <- fj_final %>%\n  mutate (Year_month = as.yearmon(timestamp))\n\nfj_final <- transform(fj_final, Year_month = as.Date(Year_month, frac = 0))\n\nhead(fj_final)\n\n\n  participantId  timestamp  category     amount Year_month\n1             0 2022-03-01      Wage 2472.50756 2022-03-01\n2             0 2022-03-01   Shelter -554.98862 2022-03-01\n3             0 2022-03-01 Education  -38.00538 2022-03-01\n4             1 2022-03-01      Wage 2046.56221 2022-03-01\n5             1 2022-03-01   Shelter -554.98862 2022-03-01\n6             1 2022-03-01 Education  -38.00538 2022-03-01\n\n\nFurther, most of the values are negative as they reflect expenditures for food, education, recreation and shelter. The positive amounts are for wage and rent adjustment. The data will also be changed from long to wide format for easier reading using the pivot_wider() function.\n\n\nShow code\nfj_final$amount <- abs(fj_final$amount)\nfj_final$amount <- round(fj_final$amount,digits=0)\n\nfj_wide <- fj_final %>%\n  group_by(participantId, Year_month, category) %>%\n  summarise(Total = sum(amount)) %>%\n  pivot_wider(names_from = category, values_from = Total)\n\n# Replace NA values with 0 for Rent Adjustment\nfj_wide[is.na(fj_wide)] <- 0\n\nhead(fj_wide)\n\n\nTo find out the cost of living and total income of the residents, the values of the expenses for the four categories, and the wage and rent adjustment will be summed up respectively. We will also derive the total savings i.e. Total income - Cost of living.\n\n\nShow code\nfj_wide$Cost_of_living <- fj_wide$Education + \n  fj_wide$Food + fj_wide$Recreation + \n  fj_wide$Shelter\n\nfj_wide$Total_Income <- fj_wide$Wage + \n  fj_wide$RentAdjustment\n\nfj_wide$Total_Savings <- fj_wide$Total_Income - fj_wide$Cost_of_living\n\n\nLastly, we will rename the participantId column in place for consistency.\n\n\nShow code\nfj_wide <- fj_wide %>%\n  rename('ParticipantID' = 'participantId')\n\n# Check if the changes are reflected correctly\ncolnames(fj_wide)\n\n\n\n\n2.3.3 Combining the Datasets\nNow that data cleaning is done, let’s merge the datasets! We need to ensure that for the participants.csv, we only keep the distinct participants that were in our fj_wide csv. Let’s see what the final dataset looks like with the following code chunk.\n\n\nShow code\nmerged <- merge(fj_wide, ppt, by = \"ParticipantID\", all.x = TRUE)\n\nfinal <- subset(merged, ParticipantID %in% fj_wide$ParticipantID)\n\nDT::datatable(final, class = \"compact\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#visualisations-insights",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#visualisations-insights",
    "title": "Demographics & Financial Analysis of City of Engagement",
    "section": "3. Visualisations & Insights",
    "text": "3. Visualisations & Insights"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#key-takeaways",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#key-takeaways",
    "title": "Demographic & Financial Analysis of City of Engagement",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThe analyses have provided the following key takeaways:\n\nThe residents are quite evenly distributed in terms of age and household size. Most of the residents have at least high school or college education, and tend to have nuclear households.\nIn terms of financial health, it is possible that most residents are just getting by, as most residents earn below $3,000 and yet experience a cost of living of about half their income. Majority of their expenses are spent on shelter.\nInterestingly, residents who earned and saved more had lower scores for happiness than those who spent more. It is possible that residents who spent more are doing so on pursuits which bring them greater joy. There was also no difference in trend amongst different age groups.\nResidents with higher levels of education (Bachelors & Graduate) tended to be happier than those who had at least high school or college education.\nResidents tended to experience a dip in happiness as they age, though the dip is not severe and thus not a cause for concern.\nMarital status and having children did not have any influence on happiness level.\n\nOfficials may wish to conduct more life courses for the residents on topics such as financial planning, mental health and wellness programmes, and ensure that older residents are able to engage in activities and social support which can ensure their mental health and therefore improve their happiness as they continue to age."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class_Ex04",
    "section": "",
    "text": "In today’s in class exercise, Prof shared with us how to combine a qqplot and tabular results of Shapiro test in a single plot."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#import-libraries",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#import-libraries",
    "title": "In-class_Ex04",
    "section": "Import libraries",
    "text": "Import libraries\nThe new libraries used today are :\n\nrstatic: Allows us to perform basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses.\ngt() : starting from a tibble table, customise a table and export in various formats. Most importantly, it works with patch. We will save the tabular results from shapiro test as gt object and export using gtsave() into .png file later.\nNote:\nImporting tidyverse: will automatically provide read_r() <- for read_csv()\n\n\npacman::p_load(rstatix, gt, patchwork,tidyverse,nortest,webshot2)\n\n\nexam <- read_csv('data/Exam_data.csv')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#background-info",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#background-info",
    "title": "In-class_Ex04",
    "section": "Background info",
    "text": "Background info\n\nThe Anderson_darling test\nUsually, when we check for normality of a distribution, we can use the Anderson-darling test or the Shapiro test. Hitting the three commands below will give us the results, but no visualisation.\n\nad.test(exam$ENGLISH)\n\n\n    Anderson-Darling normality test\n\ndata:  exam$ENGLISH\nA = 4.3661, p-value = 7.341e-11\n\n\n\n\nThe shapiro.test\nUsing shapiro.test will generate result as a HTML object.\n\nshapiro.test(exam$ENGLISH)\n\n\n    Shapiro-Wilk normality test\n\ndata:  exam$ENGLISH\nW = 0.9543, p-value = 1.811e-08\n\n\nUsing shapiro_test will generate result as a tibble object.\n\nexam %>% \n  shapiro_test(ENGLISH)\n\n# A tibble: 1 × 3\n  variable statistic            p\n  <chr>        <dbl>        <dbl>\n1 ENGLISH      0.954 0.0000000181\n\n\n\n\nQQplot\nWe can also generate the qqplot to check for normality. However qqplot does not print any p-values.\n\nggplot(exam,\n       aes(sample=ENGLISH)) +  #<<< use a new argument call sample: el scores\n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#task-for-today",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#task-for-today",
    "title": "In-class_Ex04",
    "section": "Task for today:",
    "text": "Task for today:\n\nCombine qqplot with results from Shapiro-test.\nRecall that in hands-on 3, we use DT to create an interactive table , however it is not recognized by patchwork.\nWe start by storing the shapiro test in a tibble table as shown above. Then we will use the gt() package and export it as a .png using gtsave().\n\nqq <- ggplot(exam,\n       aes(sample=ENGLISH)) +  #<<< use a new argument call sample: el scores\n  stat_qq() +\n  stat_qq_line()\n\nsw_t <- exam %>% \n  shapiro_test(ENGLISH) %>% gt()   #<<< make into a gt format (will give a nice table)  shapiro.test is not used here as it gives output in another format.\n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n\n\n\n\nI tried to customise the gt() table.\n\nqq <- ggplot(exam,\n       aes(sample=ENGLISH)) +  #<<< use a new argument call sample: el scores\n  stat_qq() +\n  stat_qq_line()\n\nsw_t <- exam %>% \n  shapiro_test(ENGLISH) %>% gt()  %>%  \n  tab_header(\n    title = 'Shapiro Test for Normality',\n    subtitle = 'English scores')\n\ntmp <- tempfile(fileext = '.png') # create  temp table\ngtsave(sw_t, tmp)  # use gtsave() to save sw_t into tmp folder\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png  # use patchwork to stitch\n\n\n\n\nThe results of the Shapiro test shows that p-value < 0.05 and we have enough statistical evidence to reject the null hypothesis and conclude that English scores do not follow normal distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-Class Exercise 5",
    "section": "",
    "text": "Today’s hands-on exercise is about network visualisation, where four network data modelling and visualisation packages will be installed and launched."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#the-dataset",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#the-dataset",
    "title": "In-Class Exercise 5",
    "section": "1.1 The Dataset",
    "text": "1.1 The Dataset\nThe dataset used are from an oil exploration and extraction company. There are two data sets. One contains the data on nodes and the other contains the edges (also know as links) data.\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\nLet’s first load the packages and datasets to be used.\n\n\nShow code\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\nShow code\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\", show_col_types = FALSE)\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#cleaning-the-dataset",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#cleaning-the-dataset",
    "title": "In-Class Exercise 5",
    "section": "1.2 Cleaning The Dataset",
    "text": "1.2 Cleaning The Dataset\nWe will examine the structure of the data frame using glimpse() of dplyr.\n\n\nShow code\nglimpse(GAStech_edges)\n\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. Before we continue, it is important for us to change the data type of SentDate field to “Date”” data type.The code chunk below will be used to perform the changes.\n\n\nShow code\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation. In view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week. Take note to (i) ungroup after you have used the group_by() function so that you can alter how you want to use group_by() for subsequent code chunks, and (ii) filter only those records where the source does not equal to target to ensure only unique interactions will be visualised later.\n\n\nShow code\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n\nNow let’s review the aggregated dataset. It looks great, good job!\n\n\nShow code\nglimpse(GAStech_edges_aggregated)\n\n\nRows: 1,372\nColumns: 4\n$ source  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday <ord> Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  <int> 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#network-objects",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#network-objects",
    "title": "In-Class Exercise 5",
    "section": "1.3 Network Objects",
    "text": "1.3 Network Objects\nWe will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame. Let’s review how the graph looks.\n\n\nShow code\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\nShow code\nGAStech_graph\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands on Ex 5",
    "section": "",
    "text": "Today’s hands-on exercise is about network visualisation, where four network data modelling and visualisation packages will be installed and launched. The objectives are as follows:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-dataset",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-dataset",
    "title": "Hands on Ex 5",
    "section": "5.1 The Dataset",
    "text": "5.1 The Dataset\nThe dataset used are from an oil exploration and extraction company. There are two data sets. One contains the data on nodes and the other contains the edges (also know as links) data.\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\nLet’s first load the packages and datasets to be used.\n\n\nShow code\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\nShow code\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\", show_col_types = FALSE)\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-dataset-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-dataset-1",
    "title": "Hands on Ex 5",
    "section": "1.2 The Dataset",
    "text": "1.2 The Dataset\nWe will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. Before we continue, it is important for us to change the data type of SentDate field to “Date”” data type.The code chunk below will be used to perform the changes.\n\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation. In view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\nNow let’s review the aggregated dataset. It looks great, good job!\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday <ord> Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  <int> 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "",
    "text": "This Take-Home Exercise is part of the VAST Challenge 2023. The country of Oceanus has sought FishEye International’s help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. They hope to understand business relationships, including finding links that will help them stop IUU fishing and protect marine species that are affected by it.\nFishEye knows from past experience that companies caught fishing illegally will shut down but will then often start up again under a different name. FishEye wants your help to visualize temporal patterns so they can compare the activities of companies over time to determine if the companies have returned to their nefarious acts.\nIn line with this, this page will attempt to answer the following task under Mini-Challenge 2 of the VAST Challenge:\nUse visual analytics to identify temporal patterns for individual entities and between entities in the knowledge graph FishEye created from trade records. Categorize the types of business relationship patterns you find. Limit your response to 600 words and 6 images."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#the-dataset",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#the-dataset",
    "title": "Take Home Ex 2",
    "section": "1.1 The Dataset",
    "text": "1.1 The Dataset\nThe dataset used is from the VAST Challenge 2023. Let’s first load the packages and dataset to be used.\n\npacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, tidyverse)\n\n\n# The front part before :: is to load the package.\n\nMC1 <- jsonlite::fromJSON(\"data/MC1.json\")\n\nThe data is in list format but we need to change it into a tabular format. We also will use the select() function to reorganise the structure of the dataset.\n\nMC1_nodes <- as_tibble(MC1$nodes) %>%\n  select(id, type, country)\n\nMC1_edges <- as_tibble(MC1$links) %>% select(source, target, type, weight, key)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#visualisation",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#visualisation",
    "title": "Demographic & Financial Analysis of City of Engagement",
    "section": "Visualisation",
    "text": "Visualisation"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#what-influences-joviality",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#what-influences-joviality",
    "title": "Demographic & Financial Analysis of City of Engagement",
    "section": "3. What influences Joviality?",
    "text": "3. What influences Joviality?\nIn this section, we will first take a quick look at the demographic profile of the residents. Simple visualisations which show the distributions will be used, such as pie, line, and bar charts. Based on these, we will conduct confirmatory data analyses.\n\n3.1. Age & Education\nThe adult residents (aged 21 and above) are quite evenly distributed in the City. As for education level, almost half (48%) of the residents had at least high school or college education, and 45% had at least a Bachelor’s degree.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nShow code\n#Calculate the proportion of residents in each category\nage_category_proportion <- final %>%\n  count(Age_Category) %>%\n  mutate(prop = n/sum(n))\n\n#Plot the pie chart for Age Category\np1 <- ggplot(age_category_proportion, aes(x=\"\", y=prop, fill=Age_Category)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  ggtitle(\"Distribution of Age (%)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(fill =\"Age Category\")\n\n#Calculate the proportion of residents in each category\neducation_proportion <- final %>%\n  count(EducationLevel) %>%\n  mutate(prop = n/sum(n))\n\n#Plot the pie chart for Education level\np2 <- ggplot(education_proportion, aes(x=\"\", y=prop, fill=EducationLevel)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  ggtitle(\"Distribution of Education Level (%)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(fill =\"Education Level\")\n  \n(p1/p2)\n\n\n\n\n\n\n\n3.2. Household Size and Children\nResidents of the City tend to have nuclear households, with a maximum of 3 members in a household. From the pie chart, we can see that about one-third of the residents have children.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nShow code\n#Calculate proportion of residents in each category\nfinal$HouseholdSize <- as.factor(final$HouseholdSize)\nhousehold_proportion <- final %>%\n  count(HouseholdSize) %>%\n  mutate(prop = n / sum(n))\n\n#Plot the pie chart for HHsize Category\np1 <- ggplot(household_proportion, aes(x=\"\", y=prop, fill=HouseholdSize)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  ggtitle(\"Distribution of Households (%)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(fill =\"Household Size\")\n\n#Calculate proportion of residents for having kids\nfinal$HaveKids <- as.factor(final$HaveKids)\nkids_proportion <- final %>%\n  count(HaveKids) %>%\n  mutate(prop = n / sum(n))\n\n#Plot the pie chart for having kids\np2 <- ggplot(kids_proportion, aes(x=\"\", y=prop, fill=HaveKids)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  ggtitle(\"Distribution of Children in Households (%)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(fill =\"Have Children\")\n\np1 / p2\n\n\n\n\n\n\n\n3.3. Joviality\nOne of the aims of this analysis is to see what influences happiness."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#what-influences-income",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#what-influences-income",
    "title": "Demographic & Financial Analysis of City of Engagement",
    "section": "4. What influences income?",
    "text": "4. What influences income?\n\n4.1. Education and smth"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#demographic-financial-profiles",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#demographic-financial-profiles",
    "title": "Demographic & Financial Analysis of City of Engagement",
    "section": "Demographic & Financial Profiles",
    "text": "Demographic & Financial Profiles\nIn this section, we will first take a quick look at the demographic and financial profile of the residents. Simple visualisations which show the distributions will be used, such as pie, line, and bar charts. Based on these, we will conduct confirmatory data analyses to assess what influences happiness.\n\n3.1. Demographic: Age & Education\nThe adult residents (aged 21 and above) are quite evenly distributed in the City. As for education level, almost half (48%) of the residents had at least high school or college education, and 45% had at least a Bachelor’s degree.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n#Calculate the proportion of residents in each category\nage_category_proportion <- final %>%\n  count(Age_Category) %>%\n  mutate(prop = n/sum(n))\n\n#Plot the pie chart for Age Category\np1 <- ggplot(age_category_proportion, aes(x=\"\", y=prop, fill=Age_Category)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  ggtitle(\"Distribution of Age (%)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(fill =\"Age Category\")\n\n#Calculate the proportion of residents in each category\neducation_proportion <- final %>%\n  count(EducationLevel) %>%\n  mutate(prop = n/sum(n))\n\n#Plot the pie chart for Education level\np2 <- ggplot(education_proportion, aes(x=\"\", y=prop, fill=EducationLevel)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  ggtitle(\"Distribution of Education Level (%)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(fill =\"Education Level\")\n  \np1 / p2\n\n\n\n\n\n\n3.2. Demographic: Household Size and Children\nResidents of the City tend to have nuclear households, with a maximum of 3 members in a household. From the pie chart, we can see that about one-third of the residents have children.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n#Calculate proportion of residents in each category\nfinal$HouseholdSize <- as.factor(final$HouseholdSize)\nhousehold_proportion <- final %>%\n  count(HouseholdSize) %>%\n  mutate(prop = n / sum(n))\n\n#Plot the pie chart for HHsize Category\np1 <- ggplot(household_proportion, aes(x=\"\", y=prop, fill=HouseholdSize)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  ggtitle(\"Distribution of Households (%)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(fill =\"Household Size\")\n\n#Calculate proportion of residents for having kids\nfinal$HaveKids <- as.factor(final$HaveKids)\nkids_proportion <- final %>%\n  count(HaveKids) %>%\n  mutate(prop = n / sum(n))\n\n#Plot the pie chart for having kids\np2 <- ggplot(kids_proportion, aes(x=\"\", y=prop, fill=HaveKids)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0) +\n  theme_void() +\n  geom_text(aes(label = paste0(round(prop*100), \"%\")), position = position_stack(vjust = 0.5), size = 3) +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  ggtitle(\"Distribution of Children in Households (%)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  labs(fill =\"Have Children\")\n\np1 / p2\n\n\n\n\n\n\n3.3. Financial: Distribution of Total Income & Cost of Living\nThe line charts below show us an overview of the proportion of residents for the total income (as defined by the sum of wages and rent adjustment) and the cost of living. Both distributions are positively skewed i.e., mean is greater than the median. Majority of the residents have a total income of below $3,000 approximately, and have a cost of living of lower than $2,000.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n#Plot Total Income\ngg1<- ggplot(data=final, \n       aes(x = Total_Income)) +\n  geom_density(color=\"turquoise\",linewidth=1)+\n  theme_classic() +\n  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +\n  labs(x= \"Wage\",y= \"Density\",title=\"Distribution of Total Income\") \n\n#Plot Total Cost of Living\ngg2<- ggplot(data=final, \n       aes(x = Cost_of_living)) +\n  geom_density(color=\"skyblue1\",linewidth=1)+\n  theme_classic() +\n  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank())+\n  labs(x= \"Cost of Living\",y= \"Density\",title=\"Distribution of Cost of Living\") \n\ngg1 / gg2\n\n\n\n\nNext, let’s look at the trend of the total income and cost of living across the months using the animated graph below. We can see that primarily, monthly expenditures were fairly stable across months. As for total income, there was a spike in March 2022, but then returned to baseline amounts thereafter.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n#Change the ppt ID to numeric for continuous scale\np3 <- ggplot(final %>% mutate(ParticipantID = as.numeric(ParticipantID)),\n       aes(x = Total_Income, y = Cost_of_living, \n           size = Total_Income, \n           colour = ParticipantID)) +\n  geom_point(alpha = 0.5, \n             show.legend = FALSE) +\n  scale_color_gradientn(colours = rainbow(100)) +\n  scale_size(range = c(1, 6)) +\n  labs(title = 'Date: {frame_time}', \n       x = 'Total Income ($)',\n       y = 'Cost of Living ($)') +\n  transition_time(Year_month) +\n  theme_classic() +\n  ease_aes('linear')\n\n#Specify to animate within a specified duration\nanimate(p3, duration =20)\n\n\n\n\n\n\n3.4 Financial: Patterns of Cost of Living\nNow that we have seen the general trend of the total income and expenditures, let’s look at the patterns of cost of living. As we need the long format, we will first create a new table called Expenses from the fj_final table so as to isolate the specific expenditure categories.\n\n\nShow code\n#Create a Total column in fj_final and rename the dataframe\nfj_final2 <- fj_final %>%\n  group_by(participantId, Year_month, category) %>%\n  summarise(Total = sum(amount))\n\n#Create a new dataframe just for expenses\nexpenses <- fj_final2 %>%\n  filter(category != 'Wage') %>%\n  filter(category != 'RentAdjustment')\n\n#Check the values\nhead(expenses)\n\n\nWe can see from the boxplot below that most of the monthly expenditure was spent on shelter, followed by recreation, food, and lastly education.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = expenses,\n       aes(x = category, y = Total, fill = category))+\n  geom_boxplot() +\n  xlab(\"Category of Expenditure\") +\n  ylab(\"Amount ($)\") +\n  ggtitle(\"Distribution of Monthly Expenses\") +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette=\"Pastel2\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#what-influences-happiness",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#what-influences-happiness",
    "title": "Demographic & Financial Analysis of City of Engagement",
    "section": "What influences Happiness?",
    "text": "What influences Happiness?\n\nJoviality\nNow that we have finished exploring the demographic and financial characteristics, let’s plot the distribution of joviality and assess if it meets the normality assumption. The Shapiro-Wilk test will be used. If the p-value is significant, the variable is not normally distributed. We will then determine what tests to carry out for our confirmatory data analyses.\n\n\n\n\n\n\nImportant\n\n\n\nWe need to test the normality assumption as it determines if we will use parametric or non-parametric methods for our statistical tests. Using the wrong tests may result in erroneous results and conclusions!\n\n\nFor the testing of normality assumption and CDA, we will transform the data to a format where each row will be equivalent to 1 resident using the below code chunk.\n\n\nShow code\njoviality <- final %>%\n  #To ungroup any variables if necessary\n  ungroup() %>%\n  #To select the columns as needed\n  select(ParticipantID, Cost_of_living, Total_Income, Total_Savings, HouseholdSize,\n         HaveKids, EducationLevel, InterestGroup, age, Age_Category, Joviality) %>% \n  group_by(ParticipantID, Joviality, HouseholdSize, HaveKids, EducationLevel, InterestGroup, age, Age_Category) %>% \n  #Calculate average financial information per participant across the 15 months\n  summarise(across(c(Total_Income, Cost_of_living, Total_Savings), mean)) %>%\n  rename(\"Avg_MonthlyIncome\" = \"Total_Income\",\n         \"Avg_MonthlyCosts\" = \"Cost_of_living\",\n         \"Avg_MonthlySavings\" = \"Total_Savings\")\n\n#Round values\njoviality$Avg_MonthlyIncome <- round(joviality$Avg_MonthlyIncome,digits=0)\njoviality$Avg_MonthlyCosts <- round(joviality$Avg_MonthlyCosts,digits=0)\njoviality$Avg_MonthlySavings <- round(joviality$Avg_MonthlySavings,digits=0)\n\n#Check output of data\nDT::datatable(joviality, class = \"compact\")\n\n\n\n\n\n\n\n\n\nTesting of Normality Assumption\nAs the plot below shows, p-value is less than 0.05 i.e., the distribution of joviality is not Normal and therefore non-parametric statistical tests must be used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n#Plot QQplot for Joviality\nqq <- ggplot(joviality,\n       aes(sample=Joviality)) +  \n  stat_qq() +\n  stat_qq_line()\n\n#Run the Shapiro Wilki test\nsw_t <- shapiro_test(ppt$Joviality) %>% gt()   \n\n#Create a temp table\ntmp <- tempfile(fileext = '.png') \n# Save sw_t into tmp folder\ngtsave(sw_t, tmp)  \ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq+table_png\n\n\n\n\n\n\nDoes Financial Health influence Happiness?\nThe Harvard Business Review published research that on average, wealthier people are happier. However, how you spend, save, and think about money shapes how much joy you get from it.\nIn this vein, let’s explore if the financial health - income, savings, and expenditures - influences the happiness of residents in the City using correlation. Logically, the more income you earn, more savings you have, and less expenditures you have, should result in higher happiness. It will be interesting to assess if the age of the resident matters as well. It is possible that the older residents would tend to have higher income (due to experience), more savings, and more expenditures.\nHowever, the correlation plots below shows otherwise:\n\nFor income, there seems to be a negative trend for all age groups i.e. higher income is negatively correlated with lower joviality scores.\nFor savings, the same trend persists - a negative trend for all age groups i.e. higher savings is negatively correlated with lower joviality scores.\nFor cost of living, there is an unexpected positive trend for all age groups i.e. higher expenses is positively correlated with higher joviality scores.\n\n\nIncomeSavingsCostsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Mapping correlation of Joviality & Average Monthly Income\np4 <- plot_ly(data = joviality, x = ~Joviality, y = ~Avg_MonthlyIncome, size = ~Avg_MonthlyIncome, \n               color = ~Age_Category) %>%\n    layout(title = '<b> Correlation: Joviality and Income <b>', xaxis = list(title = 'Joviality Index'), yaxis = list(title = 'Average Income'), legend = list(title=list(text='<b> Age Category </b>')))\n\np4\n\n#Mapping correlation of Joviality & Average Monthly Savings\np5 <- plot_ly(data = joviality, x = ~Joviality, y = ~Avg_MonthlySavings, size = ~Avg_MonthlySavings, \n               color = ~Age_Category) %>%\n    layout(title = '<b> Correlation: Joviality and Savings <b>', xaxis = list(title = 'Joviality Index'), yaxis = list(title = 'Average Savings'), legend = list(title=list(text='<b> Age Category </b>')))\n\np5\n\n#Mapping correlation of Joviality & Average Monthly Expenditures\np6 <- plot_ly(data = joviality, x = ~Joviality, y = ~Avg_MonthlyCosts, size = ~Avg_MonthlyCosts, \n               color = ~Age_Category) %>%\n    layout(title = '<b> Correlation: Joviality and Cost of Living <b>', xaxis = list(title = 'Joviality Index'), yaxis = list(title = 'Average Costs'), legend = list(title=list(text='<b> Age Category </b>')))\n\np6\n\n\n\n\nThe correlation test for the above trends are significant, indicating that the trends are statistically significant. If we go back to the Harvard Business Review’s findings, it is possible that residents are indeed spending their money in ways that make them happy, and thus cost of living is positively correlated with joviality scores, and this is statistically significant.\n\nIncomeSavingsCostsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Statistical Test of Joviality & Average Monthly Income Correlation\nggscatterstats(\n  data = joviality,\n  x = Joviality,\n  y = Avg_MonthlyIncome,\n  marginal = FALSE,\n  ) + \nggtitle(\"Joviality and Average Income\") +\nlabs(x = \"Joviality Index\", y = \"Average Income\") +\ntheme_classic()\n\n#Statistical Test of Joviality & Average Monthly Savings Correlation\nggscatterstats(\n  data = joviality,\n  x = Joviality,\n  y = Avg_MonthlySavings,\n  marginal = FALSE,\n  ) + \nggtitle(\"Joviality and Average Savings\") +\nlabs(x = \"Joviality Index\", y = \"Average Savings\") +\ntheme_classic()\n\n#Statistical Test of Joviality & Average Monthly Expenditures Correlation\nggscatterstats(\n  data = joviality,\n  x = Joviality,\n  y = Avg_MonthlyCosts,\n  marginal = FALSE,\n  ) + \nggtitle(\"Joviality and Average Cost of Living\") +\nlabs(x = \"Joviality Index\", y = \"Average Cost of Living\") +\ntheme_classic()\n\n\n\n\n\n\nDoes Education and Age influence Happiness?\nIn 2016, the Time Magazine reported that older people tend to be happier than young adults, as they are much better able to brush off life’s small stressors and accumulate a valuable thing called wisdom: being emotionally stable and compassionate, knowing yourself and being able to make smart social decisions. A year earlier, Time also published a report stating that there is no link between education level and happiness. It will be interesting to see if these two trends hold true for residents in the City.\n\nEducation and Happiness\nSince the variable joviality did not meet the normality assumption, the non-parametric Kruskal Wallis test was conducted to find if mean joviality scores differed between different levels of education. As shown below, the p-value of the Kruskal-Wallis rank sum test is 0.0036. We can conclude that mean joviality scores differed significantly across the four education levels. Specifically, High School or College education level differed significantly from Bachelors and Graduate. The mean joviality score of Bachelors and Graduate was higher than that of High School or College, indicating that the higher the level of education, the happier you are.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n#np denotes non-parametric test\nggbetweenstats(data = joviality, x = EducationLevel, y = Joviality,\n               xlab = \"Education level\", ylab = \"Joviality Index\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Mean Joviality index across Education Levels\") + \n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +\n  theme_classic() \n\n\n\n\n\n\nAge and Happiness\nIn our revamped Joviality dataset, age exists as a categorical and continuous variable. It will be interesting to see if there is a difference in our analyses for both. As the plot shows, though age is negatively correlated with joviality scores and it is statistically significant, it is a very weak correlation of only -0.07. Age as a categorical variable did not have any significant influence on joviality scores, with p-value > 0.05. Age as a continuous variable had a negative influence on joviality scores.\n\nAge (Continuous)Age CategoryCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Age as a Continuous Variable\nggscatterstats(\n  data = joviality,\n  x = Joviality,\n  y = age,\n  marginal = FALSE,\n  ) + \nggtitle(\"Joviality and Age\") +\nlabs(x = \"Joviality Index\", y = \"Age\") \n\n#Age as a Categorical Variable\nggbetweenstats(data = joviality, x = Age_Category, y = Joviality,\n               xlab = \"Age Category\", ylab = \"Joviality Index\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               sort = \"ascending\",\n               title = \"Comparison of Mean Joviality index across Age Groups\") + \n  scale_y_continuous(limits = c(0, 2)) +\n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +\n  theme_classic() \n\n\n\n\n\n\n\nDoes Marital Status and Having Kids Influence Happiness?\nGoing back to Time magazine, it reported that a study of the life satisfaction of 22 Western countries found that in many countries, parents were happier than non-parents, except for the US.\nTherefore in our final analysis, we will explore if marital status and having children influences happiness. Since all residents with a household size of 3 also satisfied the condition of true for the variable HaveKids, we will use household size as a proxy variable for marital status i.e., 1 = single, 2 = married, 3 = married with a child.\nThe plot shows that for singles, there seems to be 2 main humps at Joviality scores of 0.2 and 0.9 i.e. singles experience a varying range of happiness with some being happier than others. For majority of the married couples and those with children, joviality scores tend to be lower.\nThe Kruskal-Wallis rank sum test was non-significant with a p-value > 0.05, indicating that joviality was not influenced by marital status and having children.\n\nPlotStatistical TestCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Plot a ridgeline graph\nggplot(joviality, \n       aes(x = Joviality, \n           y = HouseholdSize,\n           fill = after_stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_x_continuous(\n    name = \"Joviality index\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_classic() +\n  scale_fill_viridis_c(name = \"Joviality\",\n                       option = \"G\")\n\n#Non-parametric Statistical Test\nggbetweenstats(data = joviality, x = HouseholdSize, y = Joviality,\n               xlab = \"Marital Status\", ylab = \"Joviality Index\",\n               type = \"np\", pairwise.comparisons = TRUE, pairwise.display = \"s\",\n               mean.ci = T, p.adjust.method = \"fdr\",  conf.level = 0.95,\n               title = \"Comparison of Mean Joviality index across Marital Status\") + \n  theme(axis.title.y=element_text(angle = 0,\n                                  vjust=0.9)) +\n  theme_classic()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-wrangling",
    "title": "Hands on Ex 5",
    "section": "5.2 Data Wrangling",
    "text": "5.2 Data Wrangling\nWe will examine the structure of the data frame using glimpse() of dplyr.\n\n\nShow code\nglimpse(GAStech_edges)\n\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. Before we continue, it is important for us to change the data type of SentDate field to “Date”” data type.The code chunk below will be used to perform the changes.\n\n\n\n\nShow code\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation. In view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\n\nShow code\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nA new field called Weight has been added in the new output file GAStech_edges_aggregated.\n\n\n\n\n\nShow code\nglimpse(GAStech_edges_aggregated)\n\n\nRows: 1,372\nColumns: 4\n$ source  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday <ord> Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  <int> 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-network-objects-using-tidygraph",
    "title": "Hands on Ex 5",
    "section": "5.3 Creating network objects using tidygraph",
    "text": "5.3 Creating network objects using tidygraph\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow. Before getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\nA tidy hope\n\n\n5.3.1 The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n5.3.2 The dplyr verbs in tidygraph\nThe activate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\nIn the above image, the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n5.3.3 Using tbl_graph() to build tidygraph data model\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph().\n\n\nShow code\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n5.3.4 Reviewing the output tidygraph’s graph object\n\n\nShow code\nGAStech_graph\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 1372 edges. The command also prints the first six rows of the node data and the first three rows of edge data. It states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n5.3.5 Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange(), for example as below. Visit the reference guide of activate() to find out more about the function.\n\n\nShow code\nGAStech_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,372 × 4\n   from    to Weekday  Weight\n  <int> <int> <ord>     <int>\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# ℹ 1,366 more rows\n#\n# A tibble: 54 × 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands on Ex 5",
    "section": "5.4 Plotting Static Network Graphs with ggraph package",
    "text": "5.4 Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs. As in all network graphs, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges, and\nlayouts.\n\n\n5.4.1 Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\n\nShow code\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n5.4.2 Changing the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read its reference guide at least once.\n\n\nShow code\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n5.4.3 Changing the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\n\nShow code\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n5.4.4 Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n \n\n\n5.4.5 Fruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout. The layout argument is used to define the layout to be used.\n\n\nShow code\ng <- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n5.4.6 Modifying network nodes\nIn this section, you will colour each node by referring to their respective departments. The geom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chunks above colour and size are used.\n\n\nShow code\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n5.4.7 Modifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable. The geom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\nShow code\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-facet-graphs",
    "title": "Hands on Ex 5",
    "section": "5.5 Creating facet graphs",
    "text": "5.5 Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data. There are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n5.5.1 Working with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\n\nShow code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n5.5.2 A framed facet graph\nThe code chunk below adds frame to each graph.\n\n\nShow code\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n5.5.3 Working with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#network-metrics-analysis",
    "title": "Hands on Ex 5",
    "section": "5.6 Network Metrics Analysis",
    "text": "5.6 Network Metrics Analysis\n\n5.6.1 Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of these network measures.\n\n\nShow code\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nThe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n5.6.2 Visualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nShow code\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n5.6.3 Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph.\nIn the code chunk below group_edge_betweenness() is used.\n\n\nShow code\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands on Ex 5",
    "section": "5.7 Building Interactive Network Graph with visNetwork",
    "text": "5.7 Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n5.7.1 Data preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\n\nShow code\nGAStech_edges_aggregated <- GAStech_edges %>%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %>%\n  rename(from = id) %>%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 1) %>%\n  ungroup()\n\n\n\n\n5.7.2 Plotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\n\nShow code\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n5.7.3 Working with layout\nIn the code chunk below, Fruchterman and Reingold layout is used. Visit Igraph to find out more about visIgraphLayout’s argument.\n\n\nShow code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n\n5.7.4 Working with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field. The code chunk below rename Department field to group.\n\n\nShow code\nGAStech_nodes <- GAStech_nodes %>%\n  rename(group = Department) \n\n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\n\nShow code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n5.7.5 Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n\nThe argument arrows is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\nVisit Option to find out more about visEdges’s argument.\n\n\nShow code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n5.7.6 Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\nVisit Option to find out more about visOption’s argument.\n\n\nShow code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#data-wrangling",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#data-wrangling",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nLet’s first load the packages and datasets to be used.\n\n\nShow code\npacman::p_load(igraph, tidygraph, ggraph, visNetwork, lubridate, clock,\n               tidyverse, graphlayouts, jsonlite, patchwork, DT, zoo, ggiraph,\n               plotly)\n\n\n\n\nShow code\nchallenge_graph <- fromJSON(\"data/mc2_challenge_graph.json\")\n\n\nThe data is in list format but we need to change it into a tabular format. We also will use the select() function to reorganise the structure of the dataset. This is first done for the nodes in the dataset. We will use the select function to choose the columns we need, as well as re-organise the order of columns.\n\n\nShow code\nMC2_nodes <- as_tibble(challenge_graph$nodes) %>%\n  select(id, shpcountry, rcvcountry)\nMC2_nodes\n\n\n# A tibble: 34,576 × 3\n   id                                shpcountry rcvcountry\n   <chr>                             <chr>      <chr>     \n 1 AquaDelight Inc and Son's         Polarinda  Oceanus   \n 2 BaringoAmerica Marine Ges.m.b.H.  <NA>       <NA>      \n 3 Yu gan  Sea spray GmbH Industrial Oceanus    Oceanus   \n 4 FlounderLeska Marine BV           <NA>       <NA>      \n 5 Olas del Mar Worldwide            Oceanus    Oceanus   \n 6 French Crab S.p.A. Worldwide      Kondanovia Utoporiana\n 7 KisumuSeafood Brothers Ltd        <NA>       <NA>      \n 8 Panope Limited Liability Company  Vesperanda Oceanus   \n 9 hǎi dǎn Corporation Wharf         Marebak    Oceanus   \n10 NamRiver Transit A/S              <NA>       <NA>      \n# ℹ 34,566 more rows\n\n\nNext, we will do the same for the edges in the dataset. Primary analysis of the data in Excel showed that there were a large portion of missing data under the columns of valueofgoods_omu, volumeteu, and valueofgoodsusd. As such, these columns will not be selected for the data.\n\n\nShow code\nMC2_edges <- as_tibble(challenge_graph$links) %>%\n  select(source, target, arrivaldate, hscode, weightkg)\n\n\nWe will now use the glimpse() function to take a look at each of the tibble dataframes. The nodes dataframe has no issues.\n\n\nShow code\nglimpse(MC2_nodes)\n\n\nRows: 34,576\nColumns: 3\n$ id         <chr> \"AquaDelight Inc and Son's\", \"BaringoAmerica Marine Ges.m.b…\n$ shpcountry <chr> \"Polarinda\", NA, \"Oceanus\", NA, \"Oceanus\", \"Kondanovia\", NA…\n$ rcvcountry <chr> \"Oceanus\", NA, \"Oceanus\", NA, \"Oceanus\", \"Utoporiana\", NA, …\n\n\nHowever, the edges dataframe reads its arrival date as a character function. Further, the task requires temporal analysis, but it only has the date.\n\n\nShow code\nglimpse(MC2_edges)\n\n\nRows: 5,464,378\nColumns: 5\n$ source      <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son's\", …\n$ target      <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica Marine…\n$ arrivaldate <chr> \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23\", \"2…\n$ hscode      <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"470710\"…\n$ weightkg    <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, 6865,…\n\n\nThus we will change the arrivaldate field to be in date format, and extract the year and month for further analysis as in the code chunk below. The preliminary analysis in Excel also showed many duplicate records, where there was the same shipment in terms of weight, which is not possible. Thus we will also extract the distinct records.\n\n\nShow code\nMC2_edges <- MC2_edges %>%\n  mutate(ArrDate = ymd(arrivaldate)) %>%\n  mutate(Year = year(arrivaldate)) %>%\n  mutate(Month = month(arrivaldate)) %>%\n  distinct()\n\n#To check the dataset\nglimpse(MC2_edges)\n\n\nRows: 5,190,407\nColumns: 8\n$ source      <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son's\", …\n$ target      <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica Marine…\n$ arrivaldate <chr> \"2034-02-12\", \"2034-03-13\", \"2028-02-07\", \"2028-02-23\", \"2…\n$ hscode      <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"470710\"…\n$ weightkg    <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, 6865,…\n$ ArrDate     <date> 2034-02-12, 2034-03-13, 2028-02-07, 2028-02-23, 2028-09-1…\n$ Year        <dbl> 2034, 2034, 2028, 2028, 2028, 2028, 2028, 2028, 2028, 2028…\n$ Month       <dbl> 2, 3, 2, 2, 9, 10, 4, 6, 9, 9, 2, 2, 4, 4, 3, 9, 3, 3, 4, …\n\n\n\nPrepare the Edges Data Table\nExamining the dataframe shows that it comprises individual records which is not very useful for visualisation, since we would like to conduct a network analysis. As such, we will perform aggregation of the data by source, target, hscode, and Year, so that the number of transactions will be transformed to become the weight of the edge between the nodes. Since we do not want linkages with low weights, we will filter to select only the edges which have weights of more than 20.\nFurther, examination of HScodes indicate that HScode equivalent to 306170 refers to fish and crustaceans. Thus we will filter the data just for this HScode, as the other records may be shipments of other materials.\n\n\nShow code\nmc2_edges_aggregated <- MC2_edges %>%\n  filter(hscode == \"306170\") %>%\n  group_by(source, target, hscode, Year) %>%\n    summarise(weights = n()) %>%\n  filter(source!=target) %>%\n  filter(weights > 20) %>%\n  ungroup()\nglimpse(mc2_edges_aggregated)\n\n\nRows: 1,588\nColumns: 5\n$ source  <chr> \"1 Ltd. Liability Co Cargo\", \"1 Ltd. Liability Co Cargo\", \"1 L…\n$ target  <chr> \"Himachal Pradesh  BV Holdings\", \"Himachal Pradesh  BV Holding…\n$ hscode  <chr> \"306170\", \"306170\", \"306170\", \"306170\", \"306170\", \"306170\", \"3…\n$ Year    <dbl> 2028, 2029, 2030, 2030, 2031, 2033, 2034, 2029, 2030, 2029, 20…\n$ weights <int> 67, 59, 26, 148, 63, 36, 24, 29, 41, 25, 29, 26, 28, 51, 70, 4…\n\n\n\n\nPrepare the Nodes Data Table\nInstead of using the nodes data table extracted from the original dataset, we will prepare a new nodes data table by using the source and target fields of mc2_edges_aggregated data table. This is necessary to ensure that the nodes in the nodes data tables include all the source and target values.\n\n\nShow code\nid1 <- mc2_edges_aggregated %>%\n  select(source) %>%\n  rename(id = source)\nid2 <- mc2_edges_aggregated %>%\n  select(target) %>%\n  rename(id = target)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#building-an-overview-of-the-network-graph",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#building-an-overview-of-the-network-graph",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Building an Overview of the Network Graph",
    "text": "Building an Overview of the Network Graph\nFirst, we will need to prepare the data model, and to do so, we rename the source and target columns in the Edges data. Using the subsequent code chunk, we will filter the nodes from the nodes data, which only exist in the edges data, and then plot our interactive network graph using the visNetwork() function.\nWhen we look at the graph, we notice that there is a ring of disconnected components within. These are probably smaller companies which do not have much network connections with other companies. However, we will retain them just in case they are useful later.\n\n\nShow code\nmc2_edges <- mc2_edges_aggregated %>%\n  rename(from = source) %>%\n  rename(to = target) %>%\n  filter(from!=to) %>%\n  ungroup()\n\n\n\nInsight 1 - Companies with Extensive Networks\nThough the graph looks too dense to provide useful information, we can actually select the ID and note how widespread the connections of each company are. If we hover over the various nodes, we can even see that there are a few companies which have extremely extensive networks:\n\nhai dan Corporation Wharf\nCaracola del Este Ltd. Liability Co\nAquaDelight N.V. Coral Reef\n\nThese companies may be connected well with other fishing companies for a few reasons:\n\nThey may be collaborating to share resources such as fishing equipment, vessels, and crew members,\nThey may have cooperative agreements to collectively fish together, or\nThey may be the mediators in that they collaborate to have better market access and distribution channels. This would allow them to negotiate better deals with wholesalers, processors and distributors, thus letting them reach a wider customer base.\n\n\n\n\n\n\n\nNote\n\n\n\nNetwork graphs may seem overwhelming at first, especially for a large graph like this. Take your time to go through the nodes and explore using the interactivity feature, and see what you can learn!\n\n\n\n\nShow code\nedges <- mc2_edges \nnodes <- mc2_nodes_extracted %>%\n  filter(id %in% c(\"id\", edges$from, edges$to))\n\n\nvisNetwork(nodes, edges, main = \"An Overview of the Network Graph\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'curvedCW'),\n           shadow = FALSE,\n           dash = FALSE) %>%\n  visOptions(highlightNearest = list (enabled = TRUE, hover = TRUE),\n             nodesIdSelection = TRUE)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#visualisation-of-growth-by-year",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#visualisation-of-growth-by-year",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Visualisation of Growth by Year",
    "text": "Visualisation of Growth by Year\nSince we are interested in temporal patterns, let’s plot the connections by year, so that we can see how the layout of the graph changes. Below is the code chunk to filter the data by each year and then create the graph using the tbl_graph() and ggraph functions.\n\n\nShow code\n#Plot for 2028\nedges_2028 <- edges %>%\n  filter(Year == \"2028\")\nnodes_2028 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2028$from, edges_2028$to))\n\ngraph1 <- tbl_graph(nodes = nodes_2028,\n                       edges = edges_2028,\n                       directed = TRUE)\n\ng1 <- ggraph(graph1, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE) +\n  ggtitle (\"2028\")\n\n#Plot for 2029\nedges_2029 <- edges %>%\n  filter(Year == \"2029\")\nnodes_2029 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2029$from, edges_2029$to))\n\ngraph2 <- tbl_graph(nodes = nodes_2029,\n                       edges = edges_2029,\n                       directed = TRUE)\n\ng2 <- ggraph(graph2, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2029\")\n\n#Plot for 2030\nedges_2030 <- edges %>%\n  filter(Year == \"2030\")\nnodes_2030 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2030$from, edges_2030$to))\n\ngraph3 <- tbl_graph(nodes = nodes_2030,\n                       edges = edges_2030,\n                       directed = TRUE)\n\ng3 <- ggraph(graph3, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2030\")\n\n#Plot for 2031\nedges_2031 <- edges %>%\n  filter(Year == \"2031\")\nnodes_2031 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2031$from, edges_2031$to))\n\ngraph4 <- tbl_graph(nodes = nodes_2031,\n                       edges = edges_2031,\n                       directed = TRUE)\n\ng4 <- ggraph(graph4, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2031\")\n\n#Plot for 2032\nedges_2032 <- edges %>%\n  filter(Year == \"2032\")\nnodes_2032 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2032$from, edges_2032$to))\n\ngraph5 <- tbl_graph(nodes = nodes_2032,\n                       edges = edges_2032,\n                       directed = TRUE)\n\ng5 <- ggraph(graph5, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2032\")\n\n#Plot for 2033\nedges_2033 <- edges %>%\n  filter(Year == \"2033\")\nnodes_2033 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2033$from, edges_2033$to))\n\ngraph6 <- tbl_graph(nodes = nodes_2033,\n                       edges = edges_2033,\n                       directed = TRUE)\n\ng6 <- ggraph(graph6, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2033\")\n\n#Plot for 2034\nedges_2034 <- edges %>%\n  filter(Year == \"2034\")\nnodes_2034 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2034$from, edges_2034$to))\n\ngraph7 <- tbl_graph(nodes = nodes_2034,\n                       edges = edges_2034,\n                       directed = TRUE)\n\ng7 <- ggraph(graph7, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2034\")\n\n\nLet’s plot the graphs after the lengthy process of filtering the data using the patchwork function so that we can see all the graphs in one figure.\n\nInsight 2\nAs we analyse how the graphs change from 2028 to 2034, we can note the following:\n\nThe disconnected components undergo quite a change over the years - though most are in connections of 2-3 nodes in 2028, they expand to other nodes over the next few years, but then return to being disconnected after 2031. Possibly, these companies did have more shipments to other companies as the years went by, but over time these went back to the usual frequency of shipments.\nWhen we look at the density of the nodes, visually they seem to increase over time i.e., the number of shipping companies increase over the years.\nLooking at the edges, there’s 2 interesting things that change over time:\n\nThe number of edges seem to increase over time i.e., more companies have shipments with each other as the years go by.\nThe weight of the edges also increase over time. For example, in 2028 there is only one thick weighted edge of 400, whereas as the years go by, the magnitude of the weight increases to 600, and there are more weighted edges as well i.e., the frequency of shipments have increased over time for certain companies.\n\n\n\n\nShow code\ng1 + g2 + g3 + g4 + g5 + g6 + g7\n\n\n\n\n\nShow code\n  plot_layout(ncol=3)\n\n\n$ncol\n[1] 3\n\n$nrow\nNULL\n\n$byrow\nNULL\n\n$widths\nNULL\n\n$heights\nNULL\n\n$guides\nNULL\n\n$tag_level\nNULL\n\n$design\nNULL\n\nattr(,\"class\")\n[1] \"plot_layout\""
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#datasets",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#datasets",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Datasets",
    "text": "Datasets\nThe dataset used comprises two main files: (i) A graph in json format comprising 34,552 nodes and 5,464,092 directed edges and (ii) A bundle of 12 files with various edges. For this task, we will only use Dataset (i). It has the following properties:\n\nNodes\n\nid: Name of the company that originated (or received) the shipment\nshpcountry: Country the company most often associated with when shipping\nrcvcountry: Country the company most often associated with when receiving\ndataset: All values are “MC2”, referring to Mini Challenge 2\n\nEdges\n\narrivaldate: Date the shipment arrived at port in YYYY-MM-DD format\nhscode: Harmonized System code for the shipment\nvalueofgoods_omu: Customs-declared value of the total shipment, in Oceanus Monetary Units (OMU)\nvolumeteu: The volume of the shipment in ‘Twenty-foot equivalent units’, roughly how many 20-foot standard containers would be required.\nweightkg: The weight of the shipment in kilograms (if known)\ndataset: All values are “MC2”, referring to Mini Challenge 2\ntype: All values are “shipment”"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-the-centrality-of-the-graph",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-the-centrality-of-the-graph",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Calculating the Centrality of the Graph",
    "text": "Calculating the Centrality of the Graph\nCentrality measures are among the most widely used indices based on network data, as they reflect a node’s importance in a network. There are different types of centrality such as closeness, degree, betweenness, and eigenvector. For this task, we will focus on degree and betweenness centrality.\nA node’s degree is a count of how many edges it has, and thus the degree centrality for a node is simply its degree. In a directed graph, there is an in-degree (edges pointing TO the node), and an out-degree (edges pointing FROM the node).\nThe betweenness centrality on the other hand, is a bit more complex - it captures the extent to which a certain node allows information to flow from one part of the network to the other i.e., how important is a node in bridging different nodes together?\nWe will use the following code chunk to calculate the abovementioned two centrality measures.\n\n\nShow code\n#Create the network graph \ncentrality_graph<- tbl_graph(nodes=nodes,\n                          edges = edges,\n                          directed = TRUE)\n\n#Calculate the centrality measures\ncentrality_graph<- centrality_graph %>%\n  activate(\"nodes\") %>% \n  mutate(betweenness_centrality = centrality_betweenness(directed = TRUE)) %>% \n  mutate(in_deg_centrality = centrality_degree(weights = weights, \n                                               mode = \"in\")) %>% \n  mutate(out_deg_centrality = centrality_degree(weights = weights, \n                                               mode = \"out\"))\n\n\nDegree Centrality\nLet’s summarise the top 10 nodes which have the highest centrality for in-degree. These are the companies which have the most imports from other companies. Knowing this information can help us detect illegal fishing in a few ways:\n\nSince certain regions/countries may have a higher prevalence of IUU fishing due to weak regulations, examining the import patterns can identify companies that consistently source seafood from these high-risk regions.\nThis information can be shared with other governments, enforcement agencies, and industry stakeholders to check if these companies also have similarly significant number of imports.\nOfficials can track the seafood supply chain for these companies so as to identify any potential gaps or discrepancies in the supply chain.\n\n\n\nShow code\nindeg_graph<- centrality_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(in_deg_centrality)) %>% \n  select(id,in_deg_centrality) %>% \n  head(n=10)\n\n\nDT::datatable(indeg_graph, class = \"compact\", colnames = c(\"Name of Company\",\"In-Degree Centrality\"),\n              caption = 'Table 1: Top 10 Companies by In-Degree')\n\n\n\n\n\n\n\nBetweenness Centrality\nLet’s summarise the top 10 nodes which have the highest centrality for betweenness centrality.\n\nInsight XXXX\nxxx\n\n\nShow code\nbtwness_graph<- centrality_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(betweenness_centrality)) %>% \n  select(id,betweenness_centrality) %>% \n  head(n=10)\n\n\nDT::datatable(btwness_graph, class = \"compact\", colnames = c(\"Name of Company\",\"Betweenness Centrality\"),\n              caption = 'Table 3: Top 10 Companies by Betweenness Centrality')\n\n\n\n\n\n\n\n\n\nInsight 4 - Visualising Highest Number of Imports - Mar del Este CJSC\n\nNetwork Graph\nLet’s map the graph of the company with highest imports to see how its networks look like, and their shipping patterns.\nMar del Este CJSC receives imports from a total of 44 distinct companies. Hovering over each of the edge will highlight the strength of each link, which is the frequency of the shipments to Mar del.\n\n\nShow code\n#Filter the dataset \nMardel_ego<- mc2_edges_aggregated %>%\n  filter(to %in% 'Mar del Este CJSC') \n\n#Create tooltip for edges info\nMardel_ego<- Mardel_ego %>%\n  mutate(title = paste('Strength of Link = ',weights))\n\n#Filter the dataset for nodes based on edges\nME_nodes <- nodes %>%\n  filter(id %in% c(\"id\", Mardel_ego$from, Mardel_ego$to))\n\nvisNetwork(ME_nodes, Mardel_ego, main = \"Network for Mar del\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(color=list(hover = \"black\"),\n  arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'curvedCW'),\n           shadow = FALSE,\n           dash = FALSE) %>%\n  visOptions(highlightNearest = list (enabled = TRUE, hover = TRUE),\n             nodesIdSelection = TRUE)\n\n\n\n\n\n\n\n\nShipping Patterns\nLet’s now look at the shipping pattern for Mar del.\nFirst, let’s filter the data to only elicit the company with the highest number of imports, Mar del Este CJSC. We will also arrange the dataset in descending order by weights to see the company which has the highest imports to Mar del. Then we will change the Year column to be in a date format as it is currently in numeric form.\nWe will then plot a line graph to see the time series trends of shipment for this company. The company has received imports from many companies, but we are only interested in looking at the companies with perhaps the 5 highest imports to Mar del. Thus we will plot the graphs for only these 5 companies. We filter the data by referring to the tibble dataset above.\n\n\nShow code\n#Filter the dataset & arrange in descending order of weights\nMardel<- mc2_edges_aggregated %>%\n  filter(to %in% 'Mar del Este CJSC') %>%\n  arrange(desc(weights))\n\n#Change the date format\nMardel2<- Mardel %>%\n  mutate (Year = as.yearmon(Year))\nMardel2 <- transform(Mardel2, Year = as.Date(Year, frac = 0))\n\n\nLooking at the plot, we can see that there is quite a large fluctuation in the frequency of shipments over the years for all 5 companies’ shipments to Mar del. While the frequency is increasing for Wave Watchers, the others are all decreasing. Specifically, Tristen Jetty, Ola de la Costa and Blue Horizon reach a peak in 2033 and then decrease sharply in 2034. It is possible that Mar del has stopped accepting shipments from them and now considers Wave Watchers as its main supplier/partner.\nHowever, we cannot definitively classify this as a red or green flag for IUU, as Mar del may have had its reasons for shifting to Wave Watchers, such as reliability, quality, or cost. To make a more accurate assessment, additional information is necessary - we may want to evaluate the reputation and track record of the companies involved, and conduct due diligence on their fishing practices, and adherence to regulations.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n#Wave Watchers Ltd.\nm1<- ggplot(Mardel2 %>% filter(from=='Wave Watchers Ltd. Liability Co'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Wave Watchers Ltd.', \n       x = 'Year', y =\"\") \n\n#Tristen Jetty Company Solutions\nm2<- ggplot(Mardel2 %>% filter(from=='Tristen Jetty Company Solutions'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Tristen Jetty', \n       x = 'Year', y =\"\") \n\n#Chuan gou N.V. Delivery\nm3<- ggplot(Mardel2 %>% filter(from=='Chuan gou  N.V. Delivery'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Chuan gou', \n       x = 'Year',\n       y='Number of Imports to Mar Del') \n\n#Ola de la Costa Ges.m.b.H.\nm4<- ggplot(Mardel2 %>% filter(from=='Ola de la Costa Ges.m.b.H.'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Ola de la Costa', \n       x = 'Year', y =\"\") \n\n#Blue Horizon Family &\nm5<- ggplot(Mardel2 %>% filter(from=='Blue Horizon Family &'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Blue Horizon Family &', \n       x = 'Year', y =\"\") \n\ngirafe(code = print(m1 / m2 / m3 / m4 / m5),\n       width_svg = 6,\n       height_svg =10)\n\n\n\n\n\n\n\nInsight 5 - Visualising Highest Number of Exports - Blue Horizon Family\n\nNetwork Graph\nLet’s summarise the top 10 nodes which have the highest centrality for out-degree. These are the companies which carry out the most exports. Knowing this information can help us detect illegal fishing in a few ways:\n\nMonitoring and oversight can be focused on these high-volume exporters, as they may have a higher risk of being involved in illegal fishing or trading of illicit seafood products.\nWe can delve deeper into the export patterns to identify the regions and countries these companies are exporting to, and identify if there are any areas where illegal fishing practices may be more prevalent.\nKnowing which companies export the most provide officials an opportunity to be able to scrutinize their records, so that they can check if the companies are compliant with regulations, such as catch documentation schemes or sustainability certifications. This will prevent the trade of illegally caught or unreported seafood.\n\n\n\nShow code\noutdeg_graph<- centrality_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(out_deg_centrality)) %>% \n  select(id,out_deg_centrality) %>% \n  head(n=10)\n\n\nDT::datatable(outdeg_graph, class = \"compact\", colnames = c(\"Name of Company\",\"Out-Degree Centrality\"),\n              caption = 'Table 2: Top 10 Companies by Out-Degree')\n\n\n\n\n\n\n\nBlue Horizon Family exports to 23 companies in total. Hovering over each of the edge will highlight the strength of each link, which is the frequency of the shipments from Blue Horizon\n\n\nShow code\n#Filter edge data\nBlue_ego<- mc2_edges_aggregated %>%\n  filter(from %in% 'Blue Horizon Family &') \n\n#Create tooltip for edges info\nBlue_ego<- Blue_ego %>%\n  mutate(title = paste('Strength of Link = ',weights))\n\n#Filter the dataset for nodes based on edges\nBlue_nodes <- nodes %>%\n  filter(id %in% c(\"id\", Blue_ego$from, Blue_ego$to))\n\n\nvisNetwork(Blue_nodes, Blue_ego, main = \"Network for Blue Horizon Family\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(color=list(hover = \"black\"), arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'curvedCW'),\n           shadow = FALSE,\n           dash = FALSE) %>%\n  visOptions(highlightNearest = list (enabled = TRUE, hover = TRUE),\n             nodesIdSelection = TRUE)\n\n\n\n\n\n\n\n\nShipping Patterns\nLet’s now look at the shipping pattern using the code chunk we did as above to only elicit the company with the highest number of exports, Blue Horizon Family.\n\n\nShow code\n#Filter the dataset & arrange in descending order of weights\nBlue<- mc2_edges_aggregated %>%\n  filter(from %in% 'Blue Horizon Family &') %>%\n  arrange(desc(weights))\n\n#Change the date format\nBlue2<- Blue %>%\n  mutate (Year = as.yearmon(Year))\nBlue2 <- transform(Blue2, Year = as.Date(Year, frac = 0))\n\n\nLooking at the plot, we can see that there is quite a large fluctuation in the frequency of shipments over the years for all 5 companies’ shipments from Blue Horizon.\nWhile the frequency is increasing for Madagascar Coast and Pao gan, the other three all peak in 2033 and then have a sharp decrease for 2024. Similar to Insight 6, it is possible that Blue Horizon now considers Madagascar Coast and Pao gan as its main supplier/partners.\nAs above, we cannot definitively classify this as a red or green flag for IUU, and additional information is necessary to make a more accurate assessment.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n#Madagascar Coast AG Freight\nb1<- ggplot(Blue2 %>% filter(to=='Madagascar Coast  AG Freight '), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Madagascar Coast AG Freight', \n       x = 'Year', y =\"\") \n\n#Mar del Este CJSC\nb2<- ggplot(Blue2 %>% filter(to=='Mar del Este CJSC'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Mar del Este CJSC', \n       x = 'Year', y =\"\") \n\n#Orange River Incorporated Shipping\nb3<- ggplot(Blue2 %>% filter(to=='Orange River   Incorporated Shipping'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Orange River Incorporated Shipping', \n       x = 'Year',\n       y='Number of Imports from Blue Horizon Family') \n\n#Costa de la Felicidad Shipping\nb4<- ggplot(Blue2 %>% filter(to=='Costa de la Felicidad Shipping'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Costa de la Felicidad Shipping', \n       x = 'Year', y =\"\") \n\n#Pao gan SE Seal\nb5<- ggplot(Blue2 %>% filter(to=='Pao gan SE Seal'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Pao gan SE Seal', \n       x = 'Year', y =\"\") \n\ngirafe(code = print(b1 / b2 / b3 / b4 / b5),\n       width_svg = 6,\n       height_svg =10)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#visualising-the-centrality-of-the-graph",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#visualising-the-centrality-of-the-graph",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Visualising the Centrality of the Graph",
    "text": "Visualising the Centrality of the Graph\nNow that we know the shipping companies with the highest number of imports and exports, let’s map their graphs to see how their networks look like, and their shipping patterns.\n\nHighest Number of Imports - Mar del Este CJSC\nMar del Este CJSC receives imports from a total of 44 distinct companies. Hovering over each of the edge will highlight the strength of each link, which is the frequency of the shipments to Mar del.\n\n\nShow code\n#Filter the dataset \nMardel_ego<- mc2_edges_aggregated %>%\n  filter(to %in% 'Mar del Este CJSC') \n\n#Create tooltip for edges info\nMardel_ego<- Mardel_ego %>%\n  mutate(title = paste('Strength of Link = ',weights))\n\n#Filter the dataset for nodes based on edges\nME_nodes <- nodes %>%\n  filter(id %in% c(\"id\", Mardel_ego$from, Mardel_ego$to))\n\nvisNetwork(ME_nodes, Mardel_ego, main = \"Network for Mar del\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(color=list(hover = \"black\"),\n  arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'curvedCW'),\n           shadow = FALSE,\n           dash = FALSE) %>%\n  visOptions(highlightNearest = list (enabled = TRUE, hover = TRUE),\n             nodesIdSelection = TRUE)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-the-shipping-patterns-for-highest-imports---mar-del-este-cjsc",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-the-shipping-patterns-for-highest-imports---mar-del-este-cjsc",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Plotting the Shipping Patterns for Highest Imports - Mar del Este CJSC",
    "text": "Plotting the Shipping Patterns for Highest Imports - Mar del Este CJSC\nOur next task is to look at the shipping pattern for Mar del.\nFirst, let’s filter the data to only elicit the company with the highest number of imports, Mar del Este CJSC. We will also arrange the dataset in descending order by weights to see the company which has the highest imports to Mar del. Then we will change the Year column to be in a date format as it is currently in numeric form.\n\n\nShow code\n#Filter the dataset & arrange in descending order of weights\nMardel<- mc2_edges_aggregated %>%\n  filter(to %in% 'Mar del Este CJSC') %>%\n  arrange(desc(weights))\n\n#Change the date format\nMardel2<- Mardel %>%\n  mutate (Year = as.yearmon(Year))\nMardel2 <- transform(Mardel2, Year = as.Date(Year, frac = 0))\n\n\nWe will then plot a line graph to see the time series trends of shipment for this company. The company has received imports from many companies, but we are only interested in looking at the companies with perhaps the 5 highest imports to Mar del. Thus we will plot the graphs for only these 5 companies. We filter the data by referring to the tibble dataset above.\n\nInsight 7 - Mar del’s Main Partners\nLooking at the plot, we can see that there is quite a large fluctuation in the frequency of shipments over the years for all 5 companies’ shipments to Mar del. While the frequency is increasing for Wave Watchers, the others are all decreasing. Specifically, Tristen Jetty, Ola de la Costa and Blue Horizon reach a peak in 2033 and then decrease sharply in 2034. It is possible that Mar del has stopped accepting shipments from them and now considers Wave Watchers as its main supplier/partner.\nHowever, we cannot definitively classify this as a red or green flag for IUU, as Mar del may have had its reasons for shifting to Wave Watchers, such as reliability, quality, or cost. To make a more accurate assessment, additional information is necessary - we may want to evaluate the reputation and track record of the companies involved, and conduct due diligence on their fishing practices, and adherence to regulations.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n#Wave Watchers Ltd.\nm1<- ggplot(Mardel2 %>% filter(from=='Wave Watchers Ltd. Liability Co'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Wave Watchers Ltd.', \n       x = 'Year', y =\"\") \n\n#Tristen Jetty Company Solutions\nm2<- ggplot(Mardel2 %>% filter(from=='Tristen Jetty Company Solutions'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Tristen Jetty', \n       x = 'Year', y =\"\") \n\n#Chuan gou N.V. Delivery\nm3<- ggplot(Mardel2 %>% filter(from=='Chuan gou  N.V. Delivery'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Chuan gou', \n       x = 'Year',\n       y='Number of Imports to Mar Del') \n\n#Ola de la Costa Ges.m.b.H.\nm4<- ggplot(Mardel2 %>% filter(from=='Ola de la Costa Ges.m.b.H.'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Ola de la Costa', \n       x = 'Year', y =\"\") \n\n#Blue Horizon Family &\nm5<- ggplot(Mardel2 %>% filter(from=='Blue Horizon Family &'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Blue Horizon Family &', \n       x = 'Year', y =\"\") \n\ngirafe(code = print(m1 / m2 / m3 / m4 / m5),\n       width_svg = 6,\n       height_svg =10)\n\n\n\n\n\n\nHighest Number of Exports - Blue Horizon Family\nBlue Horizon Family exports to 23 companies in total. Hovering over each of the edge will highlight the strength of each link, which is the frequency of the shipments from Blue Horizon\n\n\nShow code\n#Filter edge data\nBlue_ego<- mc2_edges_aggregated %>%\n  filter(from %in% 'Blue Horizon Family &') \n\n#Create tooltip for edges info\nBlue_ego<- Blue_ego %>%\n  mutate(title = paste('Strength of Link = ',weights))\n\n#Filter the dataset for nodes based on edges\nBlue_nodes <- nodes %>%\n  filter(id %in% c(\"id\", Blue_ego$from, Blue_ego$to))\n\n\nvisNetwork(Blue_nodes, Blue_ego, main = \"Network for Blue Horizon Family\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(color=list(hover = \"black\"), arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'curvedCW'),\n           shadow = FALSE,\n           dash = FALSE) %>%\n  visOptions(highlightNearest = list (enabled = TRUE, hover = TRUE),\n             nodesIdSelection = TRUE)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-the-shipping-patterns-for-highest-exports---xxx",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-the-shipping-patterns-for-highest-exports---xxx",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Plotting the Shipping Patterns for Highest Exports - XXX ",
    "text": "Plotting the Shipping Patterns for Highest Exports - XXX \nNext, let’s filter the data to only elicit the company with the highest number of exports, XXX."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-the-shipping-patterns-for-highest-exports---blue-horizon-family",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-the-shipping-patterns-for-highest-exports---blue-horizon-family",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Plotting the Shipping Patterns for Highest Exports - Blue Horizon Family",
    "text": "Plotting the Shipping Patterns for Highest Exports - Blue Horizon Family\nNext, let’s plot the shipping patterns using the code chunk we did as above to only elicit the company with the highest number of exports, Blue Horizon Family.\n\n\nShow code\n#Filter the dataset & arrange in descending order of weights\nBlue<- mc2_edges_aggregated %>%\n  filter(from %in% 'Blue Horizon Family &') %>%\n  arrange(desc(weights))\n\n#Change the date format\nBlue2<- Blue %>%\n  mutate (Year = as.yearmon(Year))\nBlue2 <- transform(Blue2, Year = as.Date(Year, frac = 0))\n\n\n\nInsight 8 - Blue Horizon’s Main Partners\nLooking at the plot, we can see that there is quite a large fluctuation in the frequency of shipments over the years for all 5 companies’ shipments from Blue Horizon.\nWhile the frequency is increasing for Madagascar Coast and Pao gan, the other three all peak in 2033 and then have a sharp decrease for 2024. Similar to Insight 6, it is possible that Blue Horizon now considers Madagascar Coast and Pao gan as its main supplier/partners.\nAs above, we cannot definitively classify this as a red or green flag for IUU, and additional information is necessary to make a more accurate assessment.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n#Madagascar Coast AG Freight\nb1<- ggplot(Blue2 %>% filter(to=='Madagascar Coast  AG Freight '), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Madagascar Coast AG Freight', \n       x = 'Year', y =\"\") \n\n#Mar del Este CJSC\nb2<- ggplot(Blue2 %>% filter(to=='Mar del Este CJSC'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Mar del Este CJSC', \n       x = 'Year', y =\"\") \n\n#Orange River Incorporated Shipping\nb3<- ggplot(Blue2 %>% filter(to=='Orange River   Incorporated Shipping'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Orange River Incorporated Shipping', \n       x = 'Year',\n       y='Number of Imports from Blue Horizon Family') \n\n#Costa de la Felicidad Shipping\nb4<- ggplot(Blue2 %>% filter(to=='Costa de la Felicidad Shipping'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Costa de la Felicidad Shipping', \n       x = 'Year', y =\"\") \n\n#Pao gan SE Seal\nb5<- ggplot(Blue2 %>% filter(to=='Pao gan SE Seal'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Pao gan SE Seal', \n       x = 'Year', y =\"\") \n\ngirafe(code = print(b1 / b2 / b3 / b4 / b5),\n       width_svg = 6,\n       height_svg =10)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#key-takeaways",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#key-takeaways",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThe analyses have provided the following key takeaways:\n\nLooking at the overall data, quite alot of companies had low betweenness, and in and out degree centrality. It’s possible that the shipment networks in this industry is quite sparse and potentially disconnected.\nOver time, there has been a growth in the shipping networks in terms of connections built, and the frequency of shipments. It is worth monitoring the rate of growth so as to earmark sudden increases for potential IUU.\nThe companies with the highest imports and exports are possible the biggest stakeholders of this industry, as they are receiving (i.e., buyer) or shipping (i.e., supplier) the most. We also plotted their 5 top partners. Though we could not definitively classify them as red/green flags for IUU, it invites great opportunities for more analysis - we may wish to zoom into the trading patterns, changes in weight of the shipments over time, as well as possibly look at communities within the companies and their behaviours. This can help to improve detection of IUU."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#visualisation-of-growth-of-the-network-over-time",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#visualisation-of-growth-of-the-network-over-time",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Visualisation of Growth of the Network Over Time",
    "text": "Visualisation of Growth of the Network Over Time\nSince we are interested in temporal patterns, let’s plot the connections by year, so that we can see how the layout of the graph changes. Below is the code chunk to filter the data by each year and then create the graph using the tbl_graph() and ggraph functions.\n\n\nShow code\n#Plot for 2028\nedges_2028 <- edges %>%\n  filter(Year == \"2028\")\nnodes_2028 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2028$from, edges_2028$to))\n\ngraph1 <- tbl_graph(nodes = nodes_2028,\n                       edges = edges_2028,\n                       directed = TRUE)\n\ng1 <- ggraph(graph1, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE) +\n  ggtitle (\"2028\")\n\n#Plot for 2029\nedges_2029 <- edges %>%\n  filter(Year == \"2029\")\nnodes_2029 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2029$from, edges_2029$to))\n\ngraph2 <- tbl_graph(nodes = nodes_2029,\n                       edges = edges_2029,\n                       directed = TRUE)\n\ng2 <- ggraph(graph2, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2029\")\n\n#Plot for 2030\nedges_2030 <- edges %>%\n  filter(Year == \"2030\")\nnodes_2030 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2030$from, edges_2030$to))\n\ngraph3 <- tbl_graph(nodes = nodes_2030,\n                       edges = edges_2030,\n                       directed = TRUE)\n\ng3 <- ggraph(graph3, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2030\")\n\n#Plot for 2031\nedges_2031 <- edges %>%\n  filter(Year == \"2031\")\nnodes_2031 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2031$from, edges_2031$to))\n\ngraph4 <- tbl_graph(nodes = nodes_2031,\n                       edges = edges_2031,\n                       directed = TRUE)\n\ng4 <- ggraph(graph4, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2031\")\n\n#Plot for 2032\nedges_2032 <- edges %>%\n  filter(Year == \"2032\")\nnodes_2032 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2032$from, edges_2032$to))\n\ngraph5 <- tbl_graph(nodes = nodes_2032,\n                       edges = edges_2032,\n                       directed = TRUE)\n\ng5 <- ggraph(graph5, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2032\")\n\n#Plot for 2033\nedges_2033 <- edges %>%\n  filter(Year == \"2033\")\nnodes_2033 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2033$from, edges_2033$to))\n\ngraph6 <- tbl_graph(nodes = nodes_2033,\n                       edges = edges_2033,\n                       directed = TRUE)\n\ng6 <- ggraph(graph6, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2033\")\n\n#Plot for 2034\nedges_2034 <- edges %>%\n  filter(Year == \"2034\")\nnodes_2034 <- nodes %>%\n  filter(id %in% c(\"id\", edges_2034$from, edges_2034$to))\n\ngraph7 <- tbl_graph(nodes = nodes_2034,\n                       edges = edges_2034,\n                       directed = TRUE)\n\ng7 <- ggraph(graph7, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(size = 2, show.legend = FALSE)+\n  ggtitle (\"2034\")\n\n\nLet’s plot the graphs below after the lengthy process of filtering the data using the patchwork function so that we can see all the graphs in one figure.\n\nInsight 2 - Change in Networks Over Time\nAs we analyse how the graphs change from 2028 to 2034, we can note the following:\n\nThe disconnected components undergo quite a change over the years - though most are in connections of 2-3 nodes in 2028, they expand to other nodes over the next few years, but then return to being disconnected after 2031. Possibly, these companies did have more shipments to other companies as the years went by, but over time these went back to the usual frequency of shipments.\nWhen we look at the density of the nodes, visually they seem to increase over time i.e., the number of shipping companies increase over the years.\nLooking at the edges, there’s 2 interesting things that change over time:\n\nThe number of edges seem to increase over time i.e., more companies have shipments with each other as the years go by.\nThe weight of the edges also increase over time. For example, in 2028 there is only one thick weighted edge of 400, whereas as the years go by, the magnitude of the weight increases to 600, and there are more weighted edges as well i.e., the frequency of shipments have increased over time for certain companies.\n\n\n\n\nShow code\ng1 + g2 + g3 + g4 + g5 + g6 + g7 +\n  plot_layout(ncol=3)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#shipping-patterns-over-time",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#shipping-patterns-over-time",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Shipping Patterns Over Time",
    "text": "Shipping Patterns Over Time\nBefore we delve into the details of specific shipping patterns (e.g., imports/exports), let’s look at the overall shipping patterns over time. For this, we will summarise the data by the year.\n\n\nShow code\n#Group the data\nsummary_shipments <- edges %>%\n  group_by(Year) %>%\n  summarize(total_freq = sum(weights)) %>%\n  ungroup() \n\n#Transform the Year to date format\nsummary_shipments<- summary_shipments %>%\n  mutate (Year = as.yearmon(Year))\nsummary_shipments <- transform(summary_shipments, Year = as.Date(Year, frac = 0))\n\n\n# Check the updated summary_yearmonth data\nprint(summary_shipments)\n\n\n        Year total_freq\n1 2028-01-01       8228\n2 2029-01-01      11066\n3 2030-01-01      11341\n4 2031-01-01      12382\n5 2032-01-01      17204\n6 2033-01-01      17691\n7 2034-01-01      18098\n\n\n\nInsight 3 - Increase in Shipments Frequency Over Time\nFrom the graph, we can see that there is an overall increasing trend - shipments have been increasing in frequency from 8228 in 2028, to 18,098 in 2034. The steepest increase was from 2031 to 2032 - this sudden sharp increase could indicate possible illegal fishing occurring in 2032. However, since this is aggregated data, we are not able to glean further detailed insights. The subsequent section will try explore the topmost exports and imports to see if there are any trends.\n\n\n\n\n\n\nNote\n\n\n\nThis is an interactive line graph. Hover over a point to see the X and Y coordinates!\n\n\n\n\nShow code\ns1 <- ggplot(summary_shipments, aes(x = Year)) +\n  geom_line(aes(y = total_freq, color = \"Total\"), size = 0.5) +\n  labs(x = \"Year\",\n       y = \"Number of Shipments\") +\n  scale_color_manual(values = c(\"Total\" = \"salmon\")) +\n  theme_classic() +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  ggtitle(\"Total Number of Shipments (2028-2034)\")\n\n#Convert to interactive line graph and hide legend\ns1 <- ggplotly(s1, tooltip = c(\"x\", \"y\"))\ns1 <- s1 %>% \n  layout(showlegend = FALSE)\ns1"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-the-centrality-of-the-graph---degree-centrality",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-the-centrality-of-the-graph---degree-centrality",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Calculating the Centrality of the Graph - Degree Centrality",
    "text": "Calculating the Centrality of the Graph - Degree Centrality\nCentrality measures are among the most widely used indices based on network data, as they reflect a node’s importance in a network. There are different types of centrality such as closeness, degree, betweenness, and eigenvector. For this task, we will focus on degree and betweenness centrality.\nA node’s degree is a count of how many edges it has, and thus the degree centrality for a node is simply its degree. In a directed graph, there is an in-degree (edges pointing TO the node), and an out-degree (edges pointing FROM the node).\nThe betweenness centrality on the other hand, is a bit more complex - it captures the extent to which a certain node allows information to flow from one part of the network to the other i.e., how important is a node in bridging different nodes together?\nWe will use the following code chunk to calculate the abovementioned two centrality measures. First, the degree centrality.\n\n\nShow code\n#Create the network graph \ncentrality_graph<- tbl_graph(nodes=nodes,\n                          edges = edges,\n                          directed = TRUE)\n\n#Calculate the centrality measures\ncentrality_graph<- centrality_graph %>%\n  activate(\"nodes\") %>% \n  mutate(in_deg_centrality = centrality_degree(weights = weights, \n                                               mode = \"in\")) %>% \n  mutate(out_deg_centrality = centrality_degree(weights = weights, \n                                               mode = \"out\"))\n\n\nLet’s summarise the top 10 nodes which have the highest centrality for in-degree. These are the companies which have the most imports from other companies. Knowing this information can help us detect illegal fishing in a few ways:\n\nSince certain regions/countries may have a higher prevalence of IUU fishing due to weak regulations, examining the import patterns can identify companies that consistently source seafood from these high-risk regions.\nThis information can be shared with other governments, enforcement agencies, and industry stakeholders to check if these companies also have similarly significant number of imports.\nOfficials can track the seafood supply chain for these companies so as to identify any potential gaps or discrepancies in the supply chain.\n\n\n\nShow code\nindeg_graph<- centrality_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(in_deg_centrality)) %>% \n  select(id,in_deg_centrality) %>% \n  head(n=10)\n\n\nDT::datatable(indeg_graph, class = \"compact\", colnames = c(\"Name of Company\",\"In-Degree Centrality\"),\n              caption = 'Table 1: Top 10 Companies by In-Degree')\n\n\n\n\n\n\n\n\nInsight 4 - Visualising Highest Number of Imports - Mar del Este CJSC\n\nNetwork Graph\nLet’s map the graph of the company with highest imports to see how its networks look like, and their shipping patterns. We need to aggregate the edges such that the frequency of shipments are collapsed over the years, as per the following code chunk.\n\n\nShow code\n#Filter the dataset \nMardel_ego<- mc2_edges %>%\n  filter(to %in% 'Mar del Este CJSC') \n\n#Group by from and to, to collapse across the years\nMardel_ego_agg <- Mardel_ego %>%\n  group_by(from, to) %>%\n  summarise(sumweights = sum(weights), .groups = 'drop') %>%\n  filter(from != to) %>%\n  arrange(desc(sumweights)) %>%\n  ungroup() \n\n#Check the data!\nMardel_ego_agg\n\n\n# A tibble: 43 × 3\n   from                            to                sumweights\n   <chr>                           <chr>                  <int>\n 1 Wave Watchers Ltd. Liability Co Mar del Este CJSC       3207\n 2 Tristen Jetty Company Solutions Mar del Este CJSC       1682\n 3 Chuan gou  N.V. Delivery        Mar del Este CJSC       1145\n 4 Blue Horizon Family &           Mar del Este CJSC        754\n 5 Mar de la Vida S.p.A.           Mar del Este CJSC        360\n 6 Tripura  Market S.A. de C.V.    Mar del Este CJSC        357\n 7 Ola de la Costa Ges.m.b.H.      Mar del Este CJSC        343\n 8 Playa de Arena OJSC Express     Mar del Este CJSC        268\n 9 shí rén yú Marine conservation  Mar del Este CJSC        267\n10 Balkan Cat ОАО Transport        Mar del Este CJSC        222\n# ℹ 33 more rows\n\n\nMar del Este CJSC receives imports from a total of 44 distinct companies. Hovering over each of the edge will highlight the strength of each link, which is the frequency of the shipments from each company to Mar del. Select the following top 3 companies to see their total frequency of shipping - Wave Watchers, Tristen Jetty and Chuan gou!\n\n\nShow code\n#Create tooltip for edges info\nMardel_ego_agg<- Mardel_ego_agg %>%\n  mutate(title = paste('Strength of Link = ',sumweights))\n\n#Filter the dataset for nodes based on edges\nME_nodes <- nodes %>%\n  filter(id %in% c(\"id\", Mardel_ego_agg$from, Mardel_ego_agg$to))\n\nvisNetwork(ME_nodes, Mardel_ego_agg, main = \"Network for Mar del\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(color=list(hover = \"black\"),\n  arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'curvedCW'),\n           shadow = FALSE,\n           dash = FALSE) %>%\n  visOptions(highlightNearest = list (enabled = TRUE, hover = TRUE),\n             nodesIdSelection = TRUE)\n\n\n\n\n\n\n\n\nShipping Patterns\nLet’s now look at the shipping pattern for Mar del.\nFirst, let’s filter the data to only elicit the company with the highest number of imports, Mar del Este CJSC. We will also arrange the dataset in descending order by weights to see the company which has the highest imports to Mar del. Then we will change the Year column to be in a date format as it is currently in numeric form.\nWe will then plot a line graph to see the time series trends of shipment for this company. The company has received imports from many companies, but we are only interested in looking at the companies with perhaps the 5 highest imports to Mar del. Thus we will plot the graphs for only these 5 companies. We filter the data by referring to the tibble dataset above.\n\n\nShow code\n#Filter the dataset & arrange in descending order of weights\nMardel<- mc2_edges %>%\n  filter(to %in% 'Mar del Este CJSC') %>%\n  arrange(desc(weights))\n\n#Change the date format\nMardel2<- Mardel %>%\n  mutate (Year = as.yearmon(Year))\nMardel2 <- transform(Mardel2, Year = as.Date(Year, frac = 0))\n\n\nLooking at the plot, we can see that there is quite a large fluctuation in the frequency of shipments over the years for all 5 companies’ shipments to Mar del. While the frequency is increasing for Wave Watchers, the others are all decreasing. Specifically, Tristen Jetty, Ola de la Costa and Blue Horizon reach a peak in 2033 and then decrease sharply in 2034. It is possible that Mar del has stopped accepting shipments from them and now considers Wave Watchers as its main supplier/partner.\nHowever, we cannot definitively classify this as a red or green flag for IUU, as Mar del may have had its reasons for shifting to Wave Watchers, such as reliability, quality, or cost. To make a more accurate assessment, additional information is necessary - we may want to evaluate the reputation and track record of the companies involved, and conduct due diligence on their fishing practices, and adherence to regulations.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n#Wave Watchers Ltd.\nm1<- ggplot(Mardel2 %>% filter(from=='Wave Watchers Ltd. Liability Co'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Wave Watchers Ltd.', \n       x = 'Year', y =\"\") \n\n#Tristen Jetty Company Solutions\nm2<- ggplot(Mardel2 %>% filter(from=='Tristen Jetty Company Solutions'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Tristen Jetty', \n       x = 'Year', y =\"\") \n\n#Chuan gou N.V. Delivery\nm3<- ggplot(Mardel2 %>% filter(from=='Chuan gou  N.V. Delivery'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Chuan gou', \n       x = 'Year',\n       y='Number of Imports to Mar Del') \n\n#Ola de la Costa Ges.m.b.H.\nm4<- ggplot(Mardel2 %>% filter(from=='Ola de la Costa Ges.m.b.H.'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Ola de la Costa', \n       x = 'Year', y =\"\") \n\n#Blue Horizon Family &\nm5<- ggplot(Mardel2 %>% filter(from=='Blue Horizon Family &'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Blue Horizon Family &', \n       x = 'Year', y =\"\") \n\ngirafe(code = print(m1 / m2 / m3 / m4 / m5),\n       width_svg = 6,\n       height_svg =10)\n\n\n\n\n\n\n\nInsight 5 - Visualising Highest Number of Exports - Blue Horizon Family\n\nNetwork Graph\nLet’s summarise the top 10 nodes which have the highest centrality for out-degree. These are the companies which carry out the most exports. Knowing this information can help us detect illegal fishing in a few ways:\n\nMonitoring and oversight can be focused on these high-volume exporters, as they may have a higher risk of being involved in illegal fishing or trading of illicit seafood products.\nWe can delve deeper into the export patterns to identify the regions and countries these companies are exporting to, and identify if there are any areas where illegal fishing practices may be more prevalent.\nKnowing which companies export the most provide officials an opportunity to be able to scrutinize their records, so that they can check if the companies are compliant with regulations, such as catch documentation schemes or sustainability certifications. This will prevent the trade of illegally caught or unreported seafood.\n\n\n\nShow code\noutdeg_graph<- centrality_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(out_deg_centrality)) %>% \n  select(id,out_deg_centrality) %>% \n  head(n=10)\n\n\nDT::datatable(outdeg_graph, class = \"compact\", colnames = c(\"Name of Company\",\"Out-Degree Centrality\"),\n              caption = 'Table 2: Top 10 Companies by Out-Degree')\n\n\n\n\n\n\n\nAs with Mar del, let’s map the graph of the company with highest exports to see how its networks look like, and their shipping patterns. We need to aggregate the edges such that the frequency of shipments are collapsed over the years, as per the following code chunk.\n\n\nShow code\n#Filter edge data\nBlue_ego<- mc2_edges %>%\n  filter(from %in% 'Blue Horizon Family &') \n\n#Group by from and to, to collapse across the years\nBlue_ego_agg <- Blue_ego %>%\n  group_by(from, to) %>%\n  summarise(sumweights = sum(weights), .groups = 'drop') %>%\n  filter(from != to) %>%\n  arrange(desc(sumweights)) %>%\n  ungroup() \n\n#Check the data!\nBlue_ego_agg\n\n\n# A tibble: 22 × 3\n   from                  to                                     sumweights\n   <chr>                 <chr>                                       <int>\n 1 Blue Horizon Family & \"Madagascar Coast  AG Freight \"              1448\n 2 Blue Horizon Family & \"Mar del Este CJSC\"                           754\n 3 Blue Horizon Family & \"Costa de la Felicidad Shipping\"              697\n 4 Blue Horizon Family & \"Orange River   Incorporated Shipping\"        646\n 5 Blue Horizon Family & \"Pao gan SE Seal\"                             347\n 6 Blue Horizon Family & \"Caracola del Sol Services\"                   332\n 7 Blue Horizon Family & \"Lake Chad  Company Ray\"                      177\n 8 Blue Horizon Family & \"hǎi dǎn Corporation Wharf\"                   154\n 9 Blue Horizon Family & \"Isla del Tesoro GmbH & Co. KG Chart\"         108\n10 Blue Horizon Family & \"Adriatic Mackerel Ges.m.b.H. Family\"          96\n# ℹ 12 more rows\n\n\nBlue Horizon Family exports to 23 companies in total. Hovering over each of the edge will highlight the strength of each link, which is the total frequency of the shipments from Blue Horizon to each company. Select the following top 3 companies to see their total frequency of shipping - Madagascar Coast, Mar del and Costa de la Felicidad!\n\n\nShow code\n#Create tooltip for edges info\nBlue_ego_agg<- Blue_ego_agg %>%\n  mutate(title = paste('Strength of Link = ',sumweights))\n\n#Filter the dataset for nodes based on edges\nBlue_nodes <- nodes %>%\n  filter(id %in% c(\"id\", Blue_ego_agg$from, Blue_ego_agg$to))\n\n\nvisNetwork(Blue_nodes, Blue_ego_agg, main = \"Network for Blue Horizon Family\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(color=list(hover = \"black\"), arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'curvedCW'),\n           shadow = FALSE,\n           dash = FALSE) %>%\n  visOptions(highlightNearest = list (enabled = TRUE, hover = TRUE),\n             nodesIdSelection = TRUE)\n\n\n\n\n\n\n\n\nShipping Patterns\nLet’s now look at the shipping pattern using the code chunk we did as above to only elicit the company with the highest number of exports, Blue Horizon Family.\n\n\nShow code\n#Filter the dataset & arrange in descending order of weights\nBlue<- mc2_edges %>%\n  filter(from %in% 'Blue Horizon Family &') %>%\n  arrange(desc(weights))\n\n#Change the date format\nBlue2<- Blue %>%\n  mutate (Year = as.yearmon(Year))\nBlue2 <- transform(Blue2, Year = as.Date(Year, frac = 0))\n\n\nLooking at the plot, we can see that there is quite a large fluctuation in the frequency of shipments over the years for all 5 companies’ shipments from Blue Horizon.\nWhile the frequency is increasing for Madagascar Coast and Pao gan, the other three all peak in 2033 and then have a sharp decrease for 2024. Similar to Insight 6, it is possible that Blue Horizon now considers Madagascar Coast and Pao gan as its main supplier/partners.\nAs above, we cannot definitively classify this as a red or green flag for IUU, and additional information is necessary to make a more accurate assessment.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n#Madagascar Coast AG Freight\nb1<- ggplot(Blue2 %>% filter(to=='Madagascar Coast  AG Freight '), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Madagascar Coast AG Freight', \n       x = 'Year', y =\"\") \n\n#Mar del Este CJSC\nb2<- ggplot(Blue2 %>% filter(to=='Mar del Este CJSC'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Mar del Este CJSC', \n       x = 'Year', y =\"\") \n\n#Orange River Incorporated Shipping\nb3<- ggplot(Blue2 %>% filter(to=='Orange River   Incorporated Shipping'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Orange River Incorporated Shipping', \n       x = 'Year',\n       y='Number of Imports from Blue Horizon Family') \n\n#Costa de la Felicidad Shipping\nb4<- ggplot(Blue2 %>% filter(to=='Costa de la Felicidad Shipping'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Costa de la Felicidad Shipping', \n       x = 'Year', y =\"\") \n\n#Pao gan SE Seal\nb5<- ggplot(Blue2 %>% filter(to=='Pao gan SE Seal'), aes(x=Year, y=weights)) +\n  geom_line( color=\"salmon\", size = 0.8) + \n  theme_classic()+\n  xlab(\"\") +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title='Pao gan SE Seal', \n       x = 'Year', y =\"\") \n\ngirafe(code = print(b1 / b2 / b3 / b4 / b5),\n       width_svg = 6,\n       height_svg =10)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-the-centrality-of-the-graph---betweenness-centrality",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-the-centrality-of-the-graph---betweenness-centrality",
    "title": "Mini Challenge 2 - VAST Challenge 2023",
    "section": "Calculating the Centrality of the Graph - Betweenness Centrality",
    "text": "Calculating the Centrality of the Graph - Betweenness Centrality\n\nInsight 6 - Visualising Highest Number of Betweenness\nWe will use the following code chunk to calculate the betweenness centrality measure.\n\n\nShow code\n#Create the network graph \nbtw_graph<- tbl_graph(nodes= mc2_nodes_extracted,\n                          edges = mc2_edges_aggregated,\n                          directed = TRUE)\n\n#Calculate the centrality measures\nbtw_graph<- btw_graph %>%\n  activate(\"nodes\") %>% \n  mutate(betweenness_centrality = centrality_betweenness(directed = TRUE))\n\n\nLet’s summarise the top 10 nodes which have the highest centrality for betweenness centrality.\nInterestingly, only one company has a value of 1 and the rest are 0. Usually, having a 0 betweenness centrality means that the graph is a complete one i.e., all nodes are connected to all others and thus there is no one node which enables better information flow than others.\nHowever, this is not logical as not all companies would be shipping to each other! Looking back at our data wrangling portion, it seems that filtering our edges file to only 1 HSCode may have strictly limited the number of nodes and edges. Thus by chance, the resulting graph was not one which had betweenness centrality.\n\n\n\n\n\n\nNote\n\n\n\nAt this point, it is wise to return to the data wrangling stage to include more HSCodes explore this. However, as this would affect the entire analysis, we will try again another time! :)\n\n\n\n\nShow code\nbtw_graph<- btw_graph %>% \n  activate(\"nodes\") %>% \n  as_tibble() %>% \n  arrange(desc(betweenness_centrality)) %>% \n  select(id,betweenness_centrality) %>% \n  head(n=10)\n\nDT::datatable(btw_graph, class = \"compact\", colnames = c(\"Name of Company\",\"Betweenness Centrality\"),\n              caption = 'Table 3: Top 10 Companies by Betweenness Centrality')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class_Ex07",
    "section": "",
    "text": "In today’s in class exercise, Prof shared with us how to plot a horizon graph.\nA horizon graph is an analytical graphical method specially designed for visualising large numbers of time-series. It aims to overcome the issue of visualising highly overlapping time-series.\nIn this section, you will learn how to plot a horizon graph by using ggHoriPlot package.\n\nImport the R Packages\n\n\nShow code\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\n\nLoading the dataset\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used.\n\n\nShow code\naverp <- read_csv(\"data/AVERP.csv\") %>%\n  mutate(`Date` = dmy(`Date`))\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to parse the Date field into appropriate Date data type in R.\n\n\n\n\nPlotting the horizon graph\nNote that there is no output for the below code chunk! Refer to the in-line comments for the logic behind the code.\n\n\nShow code\naverp %>% \n  #filter the data as necessary\n  filter(Date >= \"2018-01-01\") %>%\n  #we want to ensure only the filtered data will be placed into ggplot, so we do the above first before coding the line below\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  #we placed in ` because there's a space in the csv file for the header! So we put in the ` to ensure that it recognises as one field name. The ~ is to state it as a facet grid. \n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'BuPu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html",
    "title": "Hands-on Exercise 3A",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#tooltip-effect-with-tooltip-aesthetic",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#tooltip-effect-with-tooltip-aesthetic",
    "title": "Hands-on Exercise 3A",
    "section": "Tooltip effect with tooltip aesthetic",
    "text": "Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\nNotice that two steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\n\n\n\n\n\n\nNote\n\n\n\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed!\n\n\n\n\nShow code\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#displaying-multiple-information-on-tooltip",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#displaying-multiple-information-on-tooltip",
    "title": "Hands-on Exercise 3A",
    "section": "Displaying multiple information on tooltip",
    "text": "Displaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\n\n\n\n\n\n\nNote\n\n\n\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n\n\nShow code\nexam_data$tooltip <- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#customising-tooltip-style",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#customising-tooltip-style",
    "title": "Hands-on Exercise 3A",
    "section": "Customising Tooltip style",
    "text": "Customising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by adding css declarations.\n\n\n\n\n\n\nNote\n\n\n\nNotice that the background colour of the tooltip is white and the font colour is black and bold.\n\n\n\n\nShow code\ntooltip_css <- \"background-color:white; #<<\nfont-style:bold; color:black;\" #<<\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #<<\n    opts_tooltip(    #<<\n      css = tooltip_css)) #<<\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#displaying-statistics-on-tooltip",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#displaying-statistics-on-tooltip",
    "title": "Hands-on Exercise 3A",
    "section": "Displaying statistics on tooltip",
    "text": "Displaying statistics on tooltip\nThe code chunk below shows an advanced way to customise the tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\n\nShow code\ntooltip <- function(y, ymax, accuracy = .01) {\n  mean <- scales::number(y, accuracy = accuracy)\n  sem <- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point <- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#hover-effect-with-data_id-aesthetic",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#hover-effect-with-data_id-aesthetic",
    "title": "Hands-on Exercise 3A",
    "section": "Hover effect with data_id aesthetic",
    "text": "Hover effect with data_id aesthetic\nThe code chunk below shows the second interactive feature of ggiraph, namely data_id. Note that the default value of the hover css is hover_css = “fill:orange;”.\n\n\n\n\n\n\nNote\n\n\n\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\nShow code\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#styling-hover-effect",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#styling-hover-effect",
    "title": "Hands-on Exercise 3A",
    "section": "Styling hover effect",
    "text": "Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect. Different from previous example, in this example the css customisation request are encoded directly.\n\n\nShow code\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#combining-tooltip-and-hover-effect",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#combining-tooltip-and-hover-effect",
    "title": "Hands-on Exercise 3A",
    "section": "Combining tooltip and hover effect",
    "text": "Combining tooltip and hover effect\nSometimes, we would like to combine the tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\n\n\n\n\n\nNote\n\n\n\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS\n\n\n\n\nShow code\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#click-effect-with-onclick",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#click-effect-with-onclick",
    "title": "Hands-on Exercise 3A",
    "section": "Click effect with onclick",
    "text": "Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web, as in the code chunk below:\n\n\n\n\n\n\nNote\n\n\n\nWeb document link with a data object will be displayed on the web browser upon mouse click.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that click actions must be a string column in the dataset containing valid javascript instructions.\n\n\n\n\nShow code\nexam_data$onclick <- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#coordinated-multiple-views-with-ggiraph",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#coordinated-multiple-views-with-ggiraph",
    "title": "Hands-on Exercise 3A",
    "section": "Coordinated Multiple Views with ggiraph",
    "text": "Coordinated Multiple Views with ggiraph\nCoordinated multiple views methods will be implemented in the code chunk below.\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too!\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views\n\n\n\nShow code\np1 <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 <- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#creating-an-interactive-scatter-plot-plot_ly-method",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#creating-an-interactive-scatter-plot-plot_ly-method",
    "title": "Hands-on Exercise 3A",
    "section": "Creating an interactive scatter plot: plot_ly() method",
    "text": "Creating an interactive scatter plot: plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#working-with-visual-variable-plot_ly-method",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#working-with-visual-variable-plot_ly-method",
    "title": "Hands-on Exercise 3A",
    "section": "Working with visual variable: plot_ly() method",
    "text": "Working with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\n\n\n\n\n\nNote\n\n\n\nClick on the colour symbol at the legend to see some interactivity!\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#creating-an-interactive-scatter-plot-ggplotly-method",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#creating-an-interactive-scatter-plot-ggplotly-method",
    "title": "Hands-on Exercise 3A",
    "section": "Creating an interactive scatter plot: ggplotly() method",
    "text": "Creating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\np <- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#coordinated-multiple-views-with-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#coordinated-multiple-views-with-plotly",
    "title": "Hands-on Exercise 3A",
    "section": "Coordinated Multiple Views with plotly",
    "text": "Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\n\n\n\n\n\nNote\n\n\n\nClick on a data point of one of the scatterplots and see how the corresponding point on the other scatterplot is selected!\n\n\nThe highlight_key() creates an object of class crosstalk::SharedData, which segues us to the next section.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nd <- highlight_key(exam_data)\np1 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#interactive-data-table-dt-package",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#interactive-data-table-dt-package",
    "title": "Hands-on Exercise 3A",
    "section": "Interactive Data Table: DT package",
    "text": "Interactive Data Table: DT package\nThis is a wrapper of the JavaScript Library DataTables. Data objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nShow code\nDT::datatable(exam_data, class= \"compact\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#linked-brushing-crosstalk-method",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#linked-brushing-crosstalk-method",
    "title": "Hands-on Exercise 3A",
    "section": "Linked brushing: crosstalk method",
    "text": "Linked brushing: crosstalk method\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd <- highlight_key(exam_data) \np <- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#ggiraph",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#ggiraph",
    "title": "Hands-on Exercise 3A",
    "section": "ggiraph",
    "text": "ggiraph\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#plotly-for-r",
    "href": "Hands-on_Ex/Hands-on_Ex03A/Hands-on_Ex03A.html#plotly-for-r",
    "title": "Hands-on Exercise 3A",
    "section": "plotly for R",
    "text": "plotly for R\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly's R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html",
    "title": "Mini Challenge 3 - VAST Challenge 2023",
    "section": "",
    "text": "This Take-Home Exercise is part of the VAST Challenge 2023. The country of Oceanus has sought FishEye International’s help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. They hope to understand business relationships, including finding links that will help them stop IUU fishing and protect marine species that are affected by it.\nFishEye analysts have attempted to use traditional node-link visualizations and standard graph analyses, but these were found to be ineffective because the scale and detail in the data can obscure a business’s true structure. FishEye now wants your help to develop a new visual analytics approach to better understand fishing business anomalies.\nIn line with this, this page will attempt to answer the following task under Mini-Challenge 3 of the VAST Challenge:\nUse visual analytics to identify anomalies in the business groups present in the knowledge graph. Limit your response to 400 words and 5 images.\nDevelop a visual analytics process to find similar businesses and group them. This analysis should focus on a business’s most important features and present those features clearly to the user. Limit your response to 400 words and 5 images."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html",
    "href": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html",
    "title": "Hands-on Exercise 3B",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, we will learn how to create animated data visualisations by using gganimate and plotly r packages. At the same time, we will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoons. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, we should first ask ourselves: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#loading-the-r-packages",
    "title": "Hands-on Exercise 3B",
    "section": "Loading the R packages",
    "text": "Loading the R packages\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\n\nShow code\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#importing-the-data",
    "title": "Hands-on Exercise 3B",
    "section": "Importing the data",
    "text": "Importing the data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used. The below code chunk will import the Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\n\nShow code\ncol <- c(\"Country\", \"Continent\")\nglobalPop <- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %>%\n  mutate_each_(funs(factor(.)), col) %>%\n  mutate(Year = as.integer(Year))\n\n\nThings we can learn from the code chunk: - read_xls() of readxl package is used to import the Excel worksheet.\n\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\nUnfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\n\nShow code\ncol <- c(\"Country\", \"Continent\")\nglobalPop <- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %>%\n  mutate_at(col, as.factor) %>%\n  mutate(Year = as.integer(Year))\n\n\nInstead of using mutate_at(), across() can be used to derive the same outputs.\n\n\nShow code\ncol <- c(\"Country\", \"Continent\")\nglobalPop <- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %>%\n  mutate(across(col, as.factor)) %>%\n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#building-a-static-population-bubble-plot",
    "href": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#building-a-static-population-bubble-plot",
    "title": "Hands-on Exercise 3B",
    "section": "Building a static population bubble plot",
    "text": "Building a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\n\nShow code\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#building-the-animated-bubble-plot",
    "href": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#building-the-animated-bubble-plot",
    "title": "Hands-on Exercise 3B",
    "section": "Building the animated bubble plot",
    "text": "Building the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\n\nShow code\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#building-an-animated-bubble-plot-ggplotly-method",
    "href": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#building-an-animated-bubble-plot-ggplotly-method",
    "title": "Hands-on Exercise 3B",
    "section": "Building an animated bubble plot: ggplotly() method",
    "text": "Building an animated bubble plot: ggplotly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using ggplotly() method. The animated bubble plot includes a play/pause button and a slider component for controlling the animation. Notice that even if show.legend = FALSE argument is used, the legend would still appear on the plot. Therefore, to overcome this problem, theme(legend.position=‘none’) should be used as shown in the plot and code chunk below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\ngg <- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#building-an-animated-bubble-plot-plot_ly-method",
    "href": "Hands-on_Ex/Hands-on_Ex03B/Hands-on_Ex03B.html#building-an-animated-bubble-plot-plot_ly-method",
    "title": "Hands-on Exercise 3B",
    "section": "Building an animated bubble plot: plot_ly() method",
    "text": "Building an animated bubble plot: plot_ly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nbp <- globalPop %>%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %>%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html",
    "title": "Hands-on Exercise 4A",
    "section": "",
    "text": "Visualising distribution is not new in statistical analysis. In chapter 1 we shared some of the popular statistical graphics methods for visualising distributions, such as histogram, probability density curve (pdf), boxplot, notch plot and violin plot and how they can be created by using ggplot2. In this chapter, we will explore two relatively new statistical graphic methods for visualising distribution, namely ridgeline plot and raincloud plot by using ggplot2 and its extensions.\n\n\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist for visualising distribution and uncertainty.\n\n\n\n\n\nShow code\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\n\nShow code\nexam <- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\n\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap. The figure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\n\n\n\n\nNote\n\n\n\n\nRidgeline plots make sense when the number of groups to represent is medium to high, and a classic window separation would take too much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result e.g.if there is an obvious ranking in groups. Otherwise groups will tend to overlap each other, leading to a messy plot, thus not providing any insight.\n\n\n\n\n\nThere are several ways to plot ridgeline plot with R. In this section, we will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#E59481\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nThe figure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represents the empirical cumulative density function for the distribution of English scores.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\nThe raincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package. This produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands-on Exercise 4A",
    "section": "Visual Statistical Analysis with ggstatsplot",
    "text": "Visual Statistical Analysis with ggstatsplot\n\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#getting-started-1",
    "title": "Hands-on Exercise 4A",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and launching R packages\nIn this exercise, ggstatsplot and tidyverse will be used.\n\n\nShow code\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\nImporting data\nWe will use the exam.csv dataset.\n\n\nShow code\nexam_data <- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\nas_tibble(exam_data)\n\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   <chr>      <chr> <chr>  <chr>     <dbl> <dbl>   <dbl>\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#statistical-tests",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#statistical-tests",
    "title": "Hands-on Exercise 4A",
    "section": "Statistical Tests",
    "text": "Statistical Tests\n\nOne-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\nUnpacking the Bayes Factor\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as:\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\nHow to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\nTwo-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\nOneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\n\n\n\n\n\nImportant\n\n\n\nChange the term accordingly to display the results as necessary:\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\nBelow is a summary of between-subject tests that can be carried out for each type of analyses:\n\n\n\n\n\nSignificant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\nSignificant Test of Association (Dependence) : ggbarstats() methods\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\n\nShow code\nexam1 <- exam %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nThe code chunk below ggbarstats() is used to build a visual for Significant Test of Association.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#visualising-models",
    "title": "Hands-on Exercise 4A",
    "section": "Visualising Models",
    "text": "Visualising Models\nIn this section, you will learn how to visualise model diagnostic and model parameters by using the parameters package.\nThe Toyota Corolla case study will be used to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\nInstalling and loading the required libraries\n\n\nShow code\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n\nImporting Excel file: readxl methods\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R. Notice that the output object car_resale is a tibble data frame.\n\n\nShow code\ncar_resale <- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   <dbl> <chr>    <dbl>     <dbl>     <dbl>    <dbl>  <dbl>         <dbl>  <dbl>\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period <dbl>, HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>, …"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#regression-parameters",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#regression-parameters",
    "title": "Hands-on Exercise 4A",
    "section": "Regression Parameters",
    "text": "Regression Parameters\n\nMultiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\n\nShow code\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\nModel Diagnostic: Check for multicollinearity\nIn the below code chunk, we use the check_collinearity() of performance package.\n\n\nShow code\ncheck_c <- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\nModel Diagnostic: Check normality assumption\nIn the below code chunk, we use the check_normality() of performance package.\n\n\nShow code\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\ncheck_n <- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\nModel Diagnostic: Check model for homogeneity of variances\nIn the code chunk, check_heteroscedasticity() of performance package.\n\n\nShow code\ncheck_h <- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\nModel Diagnostic: Complete check\nWe can also perform the complete checks of the assumptions above by using check_model().\n\n\nShow code\ncheck_model(model1)\n\n\n\n\n\n\n\nVisualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nplot(parameters(model1))\n\n\n\n\n\n\nVisualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html",
    "title": "Hands-on Exercise 4B",
    "section": "",
    "text": "Visualising uncertainty is relatively new in statistical graphics. In this chapter, we will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this chapter we will be able to:\n\nplot statistics error bars by using ggplot2,\nplot interactive error bars by combining ggplot2, plotly and DT,\ncreate advanced by using ggdist, and\ncreate hypothetical outcome plots (HOPs) by using ungeviz package.\n\n\n\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process\nplotly for creating interactive plot\ngganimate for creating animation plot\nDT for displaying interactive html table\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\n\n\n\nShow code\npacman::p_load(plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\n\nShow code\nexam <- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\n\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nImportant\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample!\n\n\nIn this section, we will learn how to plot error bars of maths scores by race by using data provided in the exam tibble data frame. Firstly, code chunk below will be used to derive the necessary summary statistics. Things to note:\n\ngroup_by() of dplyr package is used to group the observation by RACE\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive the standard error of Maths by RACE, and\nthe output is saved as a tibble data table called my_sum.\n\n\nmy_sum <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\n\n\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by Race\")\n\n\n\n\n\n\n\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\nIn this section, we will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"<br>N:\", `n`,\n                                  \"<br>Avg. Scores:\", round(mean, digits = 2),\n                                  \"<br>95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /<br>maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %>%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\n\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race. Note that this function comes with many arguments, and one is advised to read the syntax reference for more details.\n\n\nShow code\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\n\nShow code\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\n\nShow code\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\nStep 1: Installing ungeviz package\n\n\nShow code\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\nStep 2: Launch the application in R\n\n\nShow code\nlibrary(ungeviz)\n\n\n\n\nShow code\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#getting-started-1",
    "title": "Hands-on Exercise 4B",
    "section": "Getting Started",
    "text": "Getting Started\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R\nFunnelPlotR for creating funnel plots\nggplot2 for creating funnel plots manually\nknitr for building static html tables and\nplotly for creating interactive funnel plots\n\n\nInstalling and loading the packages\n\n\nShow code\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nImporting the Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid-19.\n\n\nShow code\ncovid19 <- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %>%\n  mutate_if(is.character, as.factor)\nknitr::kable(head(covid19), format = 'html')\n\n\n\n\n \n  \n    Sub-district ID \n    City \n    District \n    Sub-district \n    Positive \n    Recovered \n    Death \n  \n \n\n  \n    3172051003 \n    JAKARTA UTARA \n    PADEMANGAN \n    ANCOL \n    1776 \n    1691 \n    26 \n  \n  \n    3173041007 \n    JAKARTA BARAT \n    TAMBORA \n    ANGKE \n    1783 \n    1720 \n    29 \n  \n  \n    3175041005 \n    JAKARTA TIMUR \n    KRAMAT JATI \n    BALE KAMBANG \n    2049 \n    1964 \n    31 \n  \n  \n    3175031003 \n    JAKARTA TIMUR \n    JATINEGARA \n    BALI MESTER \n    827 \n    797 \n    13 \n  \n  \n    3175101006 \n    JAKARTA TIMUR \n    CIPAYUNG \n    BAMBU APUS \n    2866 \n    2792 \n    27 \n  \n  \n    3174031002 \n    JAKARTA SELATAN \n    MAMPANG PRAPATAN \n    BANGKA \n    1828 \n    1757 \n    26"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4B",
    "section": "FunnelPlotR methods",
    "text": "FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99)\nlabel_outliers: to label outliers (true or false)\nPoisson_limits: to add Poisson limits to the plot\nOD_adjust: to add overdispersed limits to the plot\nxrange and yrange: to specify the range to display for axes, acts like a zoom function &\nOther aesthetic components such as graph title, axis labels etc\n\n\nFunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot. Things to learn from the code chunk above:\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\nShow code\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\nFunnelPlotR methods: Makeover 1\nThings to learn from the code chunk above:\n\ndata_type argument is used to change from default “SR” to “PR” (i.e. proportions)\nxrange and yrange are used to set the range of x-axis and y-axis.\n\n\n\nShow code\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #<<\n  xrange = c(0, 6500),  #<<\n  yrange = c(0, 0.05)   #<<\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\nFunnelPlotR methods: Makeover 2\nThings to learn from the code chunk above:\n\nlabel = NA argument is to removed the default label outliers feature\ntitle argument is used to add plot title\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles\n\n\n\nShow code\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #<<           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #<<\n  y_label = \"Cumulative Fatality Rate\"  #<<\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4B",
    "section": "Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, we will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance our working experience of ggplot2 to customise specialised data visualisations like funnel plots.\n\nComputing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive the cumulative death rate and standard error of cumulative death rate.\n\n\nShow code\ndf <- covid19 %>%\n  mutate(rate = Death / Positive) %>%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %>%\n  filter(rate > 0)\n\n\nNext, the fit.mean is computed by using the code chunk below.\n\n\nShow code\nfit.mean <- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nCalculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence intervals.\n\n\nShow code\nnumber.seq <- seq(1, max(df$Positive), 1)\nnumber.ll95 <- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 <- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 <- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 <- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI <- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nPlotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np <- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\nInteractive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\n\nShow code\nfp_ggplotly <- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#references",
    "title": "Hands-on Exercise 4B",
    "section": "References",
    "text": "References\n\nfunnelPlotR package\nFunnel Plots for Indirectly-standardised ratios\nChanging funnel plot options\nggplot2 package"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html",
    "href": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html",
    "title": "Hands-on Exercise 6A",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil. Its display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, we will learn how to build ternary plot programmatically using R for visualising and analysing the population structure of Singapore.\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2. The code chunks below will accomplish the task.\n\n\nShow code\npacman::p_load('plotly', 'tidyverse')\n\n\nDue to some technical issues, ggtern is currently not available for downloading via cran. We need to download ggtern from the archive by using the code chunk below. The latest archive version is 3.4.1.\n\n\nShow code\nrequire(devtools)\ninstall_version(\"ggtern\", version = \"3.4.1\", repos = \"http://cran.us.r-project.org\")\n\n\nvctrs (0.6.1 -> 0.6.2) [CRAN]\nrlang (1.1.0 -> 1.1.1) [CRAN]\n\n\nNext, load ggtern package into R environment by using the code chunk below.\n\n\nShow code\nlibrary(ggtern)\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n\nShow code\npop_data <- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nShow code\n#Deriving the young, economy active and old measures\nagpop_mutated <- pop_data %>%\n  mutate(`Year` = as.character(Year))%>%\n  spread(AG, Population) %>%\n  mutate(YOUNG = rowSums(.[4:8]))%>%\n  mutate(ACTIVE = rowSums(.[9:16]))  %>%\n  mutate(OLD = rowSums(.[17:21])) %>%\n  mutate(TOTAL = rowSums(.[22:24])) %>%\n  filter(Year == 2018)%>%\n  filter(TOTAL > 0)\n\n\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n\nShow code\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\nShow code\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n\nShow code\n# reusable function for creating annotation object\nlabel <- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis <- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes <- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %>%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#installing-and-launching-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#installing-and-launching-r-packages-1",
    "title": "Hands-on Exercise 6A",
    "section": "Installing and Launching R Packages",
    "text": "Installing and Launching R Packages\nWe will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse.\n\n\nShow code\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#data-wrangling-1",
    "href": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#data-wrangling-1",
    "title": "Hands-on Exercise 6A",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\nImporting Data\nFirst, let us import the data into R by using read_csv() of readr package. Notice that beside quality and type, the rest of the variables are numerical and continuous data type.\n\n\nShow code\nwine <- read_csv(\"data/wine_quality.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 6A",
    "section": "Building Correlation Matrix: pairs() method",
    "text": "Building Correlation Matrix: pairs() method\nThere is more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, we should read the syntax description of pairs function.\n\nBuilding a basic correlation matrix\nThe figure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\n\nShow code\npairs(wine[,1:11])\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\n\nShow code\npairs(wine[,2:12])\n\n\n\n\n\n\n\nDrawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\n\nShow code\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chunk below.\n\n\nShow code\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\nIncluding the correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now - just type this code into your R session or script. Let’s have a more fun way to display the correlation matrix.\n\n\nShow code\npanel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr <- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr <- abs(cor(x, y, use=\"complete.obs\"))\ntxt <- format(c(r, 0.123456789), digits=digits)[1]\ntxt <- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 6A",
    "section": "Visualising Correlation Matrix: ggcormat()",
    "text": "Visualising Correlation Matrix: ggcormat()\nOne of the major limitations of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To overcome this problem, the Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used. There are at least three R packages to plot corrgram:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R packages like ggstatsplot package also provides functions for building corrgram. In this section, we will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\nThe basic plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\nThings to learn from the code chunk:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\n\nggcorrplot.args argument provides additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\n\nShow code\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\nThe sample sub-code chunk below can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\n\nShow code\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#building-multiple-plots",
    "title": "Hands-on Exercise 6A",
    "section": "Building multiple plots",
    "text": "Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\nThings to learn from the code chunk:\n\nto build a facet plot, the only argument needed is grouping.var.\nBesides group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.\n\n\n\nShow code\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 6A",
    "section": "Visualising Correlation Matrix using corrplot Package",
    "text": "Visualising Correlation Matrix using corrplot Package\nIn this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too. Before getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\nGetting started with corrplot\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame. In the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\n\nShow code\nwine.cor <- cor(wine[, 1:11])\n\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\nNotice the following things:\n\nThe default visual object used to plot the corrgram is circle.\nThe default layout of the corrgram is a symmetric matrix.\nThe default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients.\nThe intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nShow code\ncorrplot(wine.cor)\n\n\n\n\n\n\n\nWorking with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter methods) which can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\nFeel free to change the method argument to other supported visual geometrics!\n\n\nShow code\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\nWorking with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\n\nShow code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\nShow code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\nWorking with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used. The figure below shows a mixed layout corrgram plotted using wine quality data.\n\n\nShow code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nThe code chunk used to plot the corrgram is shown below.\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\nShow code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\nCombining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant. With the corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\n\nShow code\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\nThe figure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\n\nShow code\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\nReorder a corrgram\nMatrix reorder is very important for mining the hidden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be overwritten by using the order argument of corrplot(). Currently, the corrplot package supports four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n\n“alphabet” for alphabetical order.\nMore algorithms can be found in the seriation package.\n\n\nShow code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\nReordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\n\nShow code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex06A/Hands-on_Ex06A.html#references",
    "title": "Hands-on Exercise 6A",
    "section": "References",
    "text": "References\n\nMichael Friendly (2002). “Corrgrams: Exploratory displays for correlation matrices”. The American Statistician, 56, 316–324.\nD.J. Murdoch, E.D. Chow (1996). “A graphical display of large correlation matrices”. The American Statistician, 50, 178–180.\nggcormat() of ggstatsplot package\nggscatmat and ggpairs of GGally.\ncorrplot. A graphical display of a correlation matrix or general matrix. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html",
    "title": "Hands-on Exercise 6B",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rows and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, we will gain hands-on experience on using R to plot static and interactive heatmaps for visualising and analysing multivariate data.\n\n\nWe will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\n\nShow code\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\n\nShow code\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below.\n\n\nShow code\nrow.names(wh) &lt;- wh$Country\n\n\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap. The code chunk below will be used to transform wh data frame into a data matrix. Notethat wh_matrix is in R matrix format.\n\n\nShow code\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\n\n\n\n\n\nThere are many R packages and functions can be used to drawing static heatmaps:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, we will plot a static heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\n\nShow code\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we have to use the default as shown in the code chunk below.\n\n\nShow code\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\n\nShow code\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\nNotice that the values are scaled now. Also note that the margins argument is used to ensure that the entire x-axis labels are displayed completely, and cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\n\n\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manual of the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\n\nShow code\nheatmaply(mtcars)\n\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create an interactive heatmap by using heatmaply package.\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\nShow code\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\n\nShow code\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nShow code\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\n\nShow code\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\n\nShow code\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\nThe figure above shows that k=3 would be good. With reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap. In the code chunk below, the Blues colour palette of rColorBrewer is used.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsize_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#installing-and-launching-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#installing-and-launching-r-packages-1",
    "title": "Hands-on Exercise 6B",
    "section": "Installing and Launching R Packages",
    "text": "Installing and Launching R Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used. The code chunk below are used to install and load the packages in R.\n\n\nShow code\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#data-preparation",
    "title": "Hands-on Exercise 6B",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn this hands-on exercise, the World Happiness 2018 data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\n\nShow code\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 6B",
    "section": "Plotting Static Parallel Coordinates Plot",
    "text": "Plotting Static Parallel Coordinates Plot\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\nPlotting simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\nShow code\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\nPlotting parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaningful understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\nThings to learn from the code chunk below:\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region names.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scales each variable so that the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\nShow code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\nParallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combine and use some of the ggplot2 functions when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\nOne of the aesthetic defects of the current design is that some of the variable names overlap on the x-axis.\n\n\nShow code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\nRotating x-axis text label\nTo make the x-axis text labels easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below.\nTo rotate x-axis text labels, we use axis.text.x as an argument to theme() function, and we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degrees.\n\n\nShow code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\nAdjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\n\nShow code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 6B",
    "section": "Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\nThe basic plot\nThe code chunk below plots an interactive parallel coordinates plot by using parallelPlot(). Notice that some of the axis labels are too long!\n\n\nShow code\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\nRotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\nOne of the useful interactive features of parallelPlot is that we can click on a variable of interest, for example Happiness score, and the monotonous blue colour (default) will change.\n\n\nShow code\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nChanging the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\n\nShow code\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nParallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot a histogram along the axis of each variable.\n\n\nShow code\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#references",
    "title": "Hands-on Exercise 6B",
    "section": "References",
    "text": "References\n\nggparcoord() of GGally package\nparcoords user guide\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#installing-and-launching-r-packages-2",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#installing-and-launching-r-packages-2",
    "title": "Hands-on Exercise 6B",
    "section": "Installing and Launching R Packages",
    "text": "Installing and Launching R Packages\nBefore we get started, we should check if treemap and tidyverse pacakges have been installed.\n\n\nShow code\npacman::p_load(treemap, treemapify, tidyverse, rmarkdown)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#data-wrangling-1",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#data-wrangling-1",
    "title": "Hands-on Exercise 6B",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\nImporting the data\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\n\nShow code\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\n\n\nData Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object, they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\n\n\n\nImportant\n\n\n\nStudents who are new to dplyr methods should consult Introduction to dplyr before moving on to the next section.\n\n\n\n\nGrouped summaries without the Pipe\nThe code chank below shows a typical two lines code approach to perform the steps.\nThe code chunk below is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\nShow code\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\n\nNote\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\n\n\nGrouped summaries with the pipe\n\n\n\n\n\n\nNote\n\n\n\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\n\nShow code\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 6B",
    "section": "Designing Treemap with treemap Package",
    "text": "Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\nDesigning a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\n\nShow code\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nUsing the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor. Things to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk below, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because its vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\n\n\nShow code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nHowever! The treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\nWorking with vColor and type arguments\nIn the code chunk below, type argument is define as value. Things to learn from the conde chunk below:\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\nShow code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nColours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping.\nThe “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color.\nThe “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\nThe “value” type treemap\nThe code chunk below shows a value type treemap. Things to learn from the code chunk below:\n\nAlthough the colour palette used is RdYlBu, there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\nShow code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nThe “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette. The code chunk below shows a manual type treemap.\n\n\nShow code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative.\nTo overcome this problem, a single colour palette such as Blues should be used.\n\n\nShow code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nTreemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\nWorking with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\n\nShow code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nUsing sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\n\nShow code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 6B",
    "section": "Designing Treemap using treemapify Package",
    "text": "Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to design treemaps closely resembling those in the previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” user guide.\n\nDesigning a basic treemap\n\n\nShow code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\nDefining hierarchy\nGroup by Planning Region:\n\n\nShow code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\nGroup by Planning Area:\n\n\nShow code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\nAdd a boundary line:\n\n\nShow code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 6B",
    "section": "Designing Interactive Treemap using d3treeR",
    "text": "Designing Interactive Treemap using d3treeR\n\nInstalling d3treeR package\nInstall the package found in github by using the codes below:\n\n\nShow code\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\n\nNow, let’s launch the package:\n\n\nShow code\nlibrary(d3treeR)\n\n\n\n\nDesigning An Interactive Treemap\nThe codes below perform two processes:\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\nShow code\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nShow code\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, we will learn how to plot functional and truthful choropleth maps by using an R package called tmap package. Besides tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package. As such, we can just load the tidyverse package. The code chunk below will be used to install and load these packages in RStudio.\n\n\nShow code\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\nTwo data sets will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nShow code\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\sherinahr\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\n\nShow code\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex. The task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\n\nShow code\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nShow code\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nShow code\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n\nShow code\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nShow code\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\n\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below. Things to learn from the code chunk:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\nShow code\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\nShow code\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing values will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map. The code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values. To add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map! The alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1). Beside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification methods.\n\n\n\n\n\n\nWarning\n\n\n\nMaps can lie!\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that in tmap, the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nShow code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\nNow, we will plot the choropleth map by using the code chunk below.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the choropleth map is shaded in greens. To reverse the colour shading, add a “-” prefix.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nMap layout refers to the combination of all map elements into a cohesive map. Map elements include the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios, among others. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style(). The code chunk below shows the classic style is used.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\nTo reset the default style, you can use the code chunk: tmap_style(\"white\")\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill():\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments:\n\n\nShow code\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\n\nShow code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\n\nShow code\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\n\nShow code\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-import",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-import",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "Two data sets will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nShow code\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\sherinahr\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\n\nShow code\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex. The task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\n\nShow code\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nShow code\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nShow code\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n\nShow code\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nShow code\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "Two approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below. Things to learn from the code chunk:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\nShow code\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\nShow code\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing values will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map. The code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values. To add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map! The alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1). Beside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification methods.\n\n\n\n\n\n\nWarning\n\n\n\nMaps can lie!\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that in tmap, the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nShow code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\nNow, we will plot the choropleth map by using the code chunk below.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the choropleth map is shaded in greens. To reverse the colour shading, add a “-” prefix.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nMap layout refers to the combination of all map elements into a cohesive map. Map elements include the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios, among others. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style(). The code chunk below shows the classic style is used.\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\nTo reset the default style, you can use the code chunk: tmap_style(\"white\")\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill():\n\n\nShow code\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments:\n\n\nShow code\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\n\nShow code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\n\nShow code\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\n\nShow code\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#references",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "tmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#loading-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#loading-packages",
    "title": "Hands-on Exercise 8",
    "section": "Loading Packages",
    "text": "Loading Packages\nBefore we get started, we need to ensure that tmap package of R and other related R packages have been installed and loaded into R.\n\n\nShow code\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#geospatial-data-wrangling-1",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#geospatial-data-wrangling-1",
    "title": "Hands-on Exercise 8",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\n\nThe data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System.\n\n\nData Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\n\nShow code\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job. Notice that the sgpools data in tibble data frame and not the common R data frame.\n\n\nShow code\nlist(sgpools) \n\n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\n\n\nCreating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\n\nShow code\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\nLook at the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\n\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below. The output shows that sgppols_sf is in point feature class. Its epsg ID is 3414. The bbox provides information of the extend of the geospatial data.\n\n\nShow code\nlist(sgpools_sf)\n\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#drawing-proportional-symbol-map",
    "title": "Hands-on Exercise 8",
    "section": "Drawing Proportional Symbol Map",
    "text": "Drawing Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used. The code chunk below will turn on the interactive mode of tmap.\n\n\nShow code\ntmap_mode(\"view\")\n\n\n\nIt all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\n\nShow code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\nLets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\n\nShow code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\nLets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\n\nShow code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n\nI have a twin brother :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\n\nShow code\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmap’s Viewer back to plot mode by using the code chunk below.\n\n\nShow code\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#references-1",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#references-1",
    "title": "Hands-on Exercise 8",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#all-about-tmap-package-1",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#all-about-tmap-package-1",
    "title": "Hands-on Exercise 8",
    "section": "All about tmap package",
    "text": "All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#geospatial-data-wrangling-2",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#geospatial-data-wrangling-2",
    "title": "Hands-on Exercise 8",
    "section": "Geospatial data wrangling",
    "text": "Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-wrangling-2",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-wrangling-2",
    "title": "Hands-on Exercise 8",
    "section": "Data wrangling",
    "text": "Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "title": "Hands-on Exercise 8",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and loading packages\n\n\nShow code\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\nImporting data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\n\nShow code\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 8",
    "section": "Basic Choropleth Mapping",
    "text": "Basic Choropleth Mapping\n\nVisualising distribution of non-functional water point\nLet’s plot a choropleth map showing the distribution of non-function water point by LGA.\n\n\nShow code\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\n\n\nShow code\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\n\n\nShow code\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 8",
    "section": "Choropleth Map for Rates",
    "text": "Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\nDeriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\n\nShow code\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\nPlotting map of rate\nLet’s plot a choropleth map showing the distribution of percentage functional water point by LGA.\n\n\nShow code\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#extreme-value-maps",
    "title": "Hands-on Exercise 8",
    "section": "Extreme Value Maps",
    "text": "Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\nPercentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the beginning and end points need to be included.\n\nData Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\n\nShow code\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\n\nStep 2: Creating customised classification and extracting values\n\n\nShow code\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\nWhy write functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\nCreating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nShow code\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nA percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\n\nShow code\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\nTest drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\n\nShow code\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\nBox map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n\nShow code\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\nCreating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\n\nShow code\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n\nCreating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nShow code\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nTest drive the newly created function\nLet’s test the newly created function!\n\n\nShow code\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\nBoxmap function\nThe code chunk below is an R function to create a box map. The arguments are:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\n\nIt will then return a tmap-element (plots a map).\n\n\nShow code\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\n\n\nShow code\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#data-import",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#data-import",
    "title": "Mini Challenge 3 - VAST Challenge 2023",
    "section": "Data Import",
    "text": "Data Import\nLet’s first load the packages and datasets to be used.\n\n\nShow code\npacman::p_load(jsonlite, tidygraph, ggraph, \n               visNetwork, graphlayouts, ggforce, \n               skimr, tidytext, tidyverse)\n\n\nIn the code chunk below, fromJSON() of jsonlite package is used to import MC3.json into R environment. Examination of the dataset shows that it is a large list R object.\n\n\nShow code\nmc3_data &lt;- fromJSON(\"data/MC3.json\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#extracting-edges",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#extracting-edges",
    "title": "Mini Challenge 3 - VAST Challenge 2023",
    "section": "Extracting Edges",
    "text": "Extracting Edges\nThe code chunk below will be used to extract the links data.frame of mc3_data and save it as a tibble data.frame called mc3_edges.\n\n\n\n\n\n\nNote\n\n\n\n\ndistinct() is used to ensure that there will be no duplicated records.\nmutate() and as.character() are used to convert the field data type from list to character.\ngroup_by() and summarise() are used to count the number of unique links.\nthe filter(source!=target) is to ensure that there are no records with similar source and target.\n\n\n\n\n\nShow code\nmc3_edges &lt;- as_tibble(mc3_data$links) %&gt;% \n  distinct() %&gt;%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type)) %&gt;%\n  group_by(source, target, type) %&gt;%\n    summarise(weights = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#extracting-nodes",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#extracting-nodes",
    "title": "Mini Challenge 3 - VAST Challenge 2023",
    "section": "Extracting Nodes",
    "text": "Extracting Nodes\nThe code chunk below will be used to extract the nodes data.frame of mc3_data and save it as a tibble data.frame called mc3_nodes.\n\n\n\n\n\n\nNote\n\n\n\n\nmutate() and as.character() are used to convert the field data type from list to character.\nTo convert revenue_omu from list data type to numeric data type, we need to convert the values into character first by using as.character(). Then, as.numeric() will be used to convert them into numeric data type.\nselect() is used to re-organise the order of the fields.\n\n\n\n\n\nShow code\nmc3_nodes &lt;- as_tibble(mc3_data$nodes) %&gt;%\n  mutate(country = as.character(country),\n         id = as.character(id),\n         product_services = as.character(product_services),\n         revenue_omu = as.numeric(as.character(revenue_omu)),\n         type = as.character(type)) %&gt;%\n  select(id, country, type, revenue_omu, product_services)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#exploring-the-edges-data-frame",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#exploring-the-edges-data-frame",
    "title": "Mini Challenge 3 - VAST Challenge 2023",
    "section": "Exploring the edges data frame",
    "text": "Exploring the edges data frame\nIn the code chunk below, skim() of skimr package is used to display the summary statistics of mc3_edges tibble data frame. The report reveals that there is no missing values.\n\n\nShow code\nskim(mc3_edges)\n\n\n\nData summary\n\n\nName\nmc3_edges\n\n\nNumber of rows\n24036\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsource\n0\n1\n6\n700\n0\n12856\n0\n\n\ntarget\n0\n1\n6\n28\n0\n21265\n0\n\n\ntype\n0\n1\n16\n16\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nweights\n0\n1\n1\n0\n1\n1\n1\n1\n1\n▁▁▇▁▁\n\n\n\n\n\nIn the code chunk below, datatable() of DT package is used to display mc3_edges tibble data frame as an interactive table.\n\n\nShow code\nDT::datatable(mc3_edges)\n\n\n\n\n\n\n\nLet’s plot a bar graph to show the type of edges. As we can see from the barchart below, there are about 16,000 edges for beneficial owner, and about 7,500 edges for company contacts.\n\n\nShow code\nggplot(data = mc3_edges,\n       aes(x = type)) +\n  geom_bar(fill=\"slategray1\") + \n  theme_classic()"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#exploring-the-nodes-data-frame",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex03.html#exploring-the-nodes-data-frame",
    "title": "Mini Challenge 3 - VAST Challenge 2023",
    "section": "Exploring the nodes data frame",
    "text": "Exploring the nodes data frame\nSimilarly, skim() of skimr package is used to display the summary statistics of mc3_nodes tibble data frame. The report reveals that there is no missing values.\n\n\nShow code\nskim(mc3_nodes)\n\n\n\nData summary\n\n\nName\nmc3_nodes\n\n\nNumber of rows\n27622\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1\n6\n64\n0\n22929\n0\n\n\ncountry\n0\n1\n2\n15\n0\n100\n0\n\n\ntype\n0\n1\n7\n16\n0\n3\n0\n\n\nproduct_services\n0\n1\n4\n1737\n0\n3244\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrevenue_omu\n21515\n0.22\n1822155\n18184433\n3652.23\n7676.36\n16210.68\n48327.66\n310612303\n▇▁▁▁▁\n\n\n\n\n\nIn the code chunk below, datatable() of DT package is used to display mc3_nodes tibble data frame as an interactive table.\n\n\nShow code\nDT::datatable(mc3_nodes)\n\n\n\n\n\n\n\nLet’s plot a bar graph to show the type of edges. As we can see from the barchart below, there are about 12,000 nodes for beneficial owners, 8,750 nodes for company, and 7,000 nodes for company contacts.\n\n\nShow code\nggplot(data = mc3_nodes,\n       aes(x = type)) +\n  geom_bar(fill=\"slategray1\") + \n  theme_classic()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608 - Sherinah Rashid",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6B",
    "section": "",
    "text": "We will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\n\nShow code\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#data-wrangling",
    "title": "Hands-on Exercise 6B",
    "section": "",
    "text": "In this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\n\nShow code\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below.\n\n\nShow code\nrow.names(wh) &lt;- wh$Country\n\n\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap. The code chunk below will be used to transform wh data frame into a data matrix. Notethat wh_matrix is in R matrix format.\n\n\nShow code\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#static-heatmap",
    "title": "Hands-on Exercise 6B",
    "section": "",
    "text": "There are many R packages and functions can be used to drawing static heatmaps:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, we will plot a static heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\n\nShow code\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we have to use the default as shown in the code chunk below.\n\n\nShow code\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\n\nShow code\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\nNotice that the values are scaled now. Also note that the margins argument is used to ensure that the entire x-axis labels are displayed completely, and cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06B/Hands-on_Ex06B.html#interactive-heatmap",
    "title": "Hands-on Exercise 6B",
    "section": "",
    "text": "heatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manual of the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\n\nShow code\nheatmaply(mtcars)\n\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create an interactive heatmap by using heatmaply package.\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\nShow code\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\n\nShow code\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nShow code\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\n\nShow code\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\n\nShow code\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\nThe figure above shows that k=3 would be good. With reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap. In the code chunk below, the Blues colour palette of rColorBrewer is used.\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsize_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\n\nShow code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  }
]